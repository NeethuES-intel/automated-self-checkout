{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Intel\u00ae Automated Self-Checkout Reference Package Overview As Computer Vision becomes more and more mainstream, especially for industrial & retail use cases, development and deployment of these solutions becomes more challenging. Vision workloads are large and complex and need to go through many stages. For instance, in the pipeline below, the video data is ingested, pre-processed before each inferencing step, inferenced using two models - YOLOv5 and EfficientNet, and post processed to generate metadata and show the bounding boxes for each frame. This pipeline is just an example of the supported models and pipelines found within this reference. Automated self-checkout solutions are complex, and retailers, independent software vendors (ISVs), and system integrators (SIs) require a good understanding of hardware and software, the costs involved in setting up and scaling the system, and the configuration that best suits their needs. Vision workloads are significantly larger and require systems to be architected, built, and deployed with several considerations. Hence, a set of ingredients needed to create an automated self-checkout solution is necessary. More details are available on the Intel Developer Focused Webpage and on this LinkedIn Blog The Intel\u00ae Automated Self-Checkout Reference Package provides critical components required to build and deploy a self-checkout use case using Intel\u00ae hardware, software, and other open-source software. This reference implementation provides a pre-configured automated self-checkout pipeline that is optimized for Intel\u00ae hardware. The solution includes profiles and optimization using Open Vino Model Server (OVMS) as shown in the figure below. The reference solution also includes a set of benchmarking tools, shown in the image below, to evaluate the workload on different hardware platforms. This reference solution will help evaluate your required hardware to minimize the cost per workload. Install Platform Make sure that your platform is included in the supported platform list . To set up the platform, refer to Hardware Setup . Releases For the project release notes, refer to the GitHub* Repository . License This project is Licensed under an Apache License .","title":"Home"},{"location":"index.html#intel-automated-self-checkout-reference-package","text":"","title":"Intel\u00ae Automated Self-Checkout Reference Package"},{"location":"index.html#overview","text":"As Computer Vision becomes more and more mainstream, especially for industrial & retail use cases, development and deployment of these solutions becomes more challenging. Vision workloads are large and complex and need to go through many stages. For instance, in the pipeline below, the video data is ingested, pre-processed before each inferencing step, inferenced using two models - YOLOv5 and EfficientNet, and post processed to generate metadata and show the bounding boxes for each frame. This pipeline is just an example of the supported models and pipelines found within this reference. Automated self-checkout solutions are complex, and retailers, independent software vendors (ISVs), and system integrators (SIs) require a good understanding of hardware and software, the costs involved in setting up and scaling the system, and the configuration that best suits their needs. Vision workloads are significantly larger and require systems to be architected, built, and deployed with several considerations. Hence, a set of ingredients needed to create an automated self-checkout solution is necessary. More details are available on the Intel Developer Focused Webpage and on this LinkedIn Blog The Intel\u00ae Automated Self-Checkout Reference Package provides critical components required to build and deploy a self-checkout use case using Intel\u00ae hardware, software, and other open-source software. This reference implementation provides a pre-configured automated self-checkout pipeline that is optimized for Intel\u00ae hardware. The solution includes profiles and optimization using Open Vino Model Server (OVMS) as shown in the figure below. The reference solution also includes a set of benchmarking tools, shown in the image below, to evaluate the workload on different hardware platforms. This reference solution will help evaluate your required hardware to minimize the cost per workload.","title":"Overview"},{"location":"index.html#install-platform","text":"Make sure that your platform is included in the supported platform list . To set up the platform, refer to Hardware Setup .","title":"Install Platform"},{"location":"index.html#releases","text":"For the project release notes, refer to the GitHub* Repository .","title":"Releases"},{"location":"index.html#license","text":"This project is Licensed under an Apache License .","title":"License"},{"location":"LICENSE.html","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work , attach the following boilerplate notice , with the fields enclosed by brackets \"[]\" replaced with your own identifying information . ( Don 't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2023 Intel Corporation Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"LICENSE"},{"location":"faq.html","text":"Frequently Asked Questions What are the platform requirements? For optimal hardware, refer to the platform guide . What are the software prerequisites? At a minimum, you will need Docker* 23.0 or later. For more details, refer to the hardware setup . How do I download the models? The models are downloaded automatically when the benchmark script benchmark.sh is run. For more details on downloading models, refer to Pipeline Setup . How do I simulate RTSP cameras? You can use the camera simulator script camera-simulator.sh to run simulated RTSP camera streams. For more details on the script, refer to Run Camera Simulator page. How do I download video files for benchmarking? You can download your own media file or use the the provided download_sample_videos.sh script to download an existing media file. For more details, refer to Benchmark Pipeline . How do I run different types of pipelines? For details on running different types of pipelines, refer to Quick Start Guide . Where are the pipeline container logs located ? Pipeline container logs are redirected to a text file at automated-self-checkout/results . For every new pipeline a new log file pipeline##.log (eg.: pipeline0.log, pipeline1.log, ... etc.) is created. Note As pipeline container logs are redirected to separate text files, docker logs displayed using portainer or command docker logs <**CONTAINER**> will be empty.","title":"FAQs"},{"location":"faq.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq.html#what-are-the-platform-requirements","text":"For optimal hardware, refer to the platform guide .","title":"What are the platform requirements?"},{"location":"faq.html#what-are-the-software-prerequisites","text":"At a minimum, you will need Docker* 23.0 or later. For more details, refer to the hardware setup .","title":"What are the software prerequisites?"},{"location":"faq.html#how-do-i-download-the-models","text":"The models are downloaded automatically when the benchmark script benchmark.sh is run. For more details on downloading models, refer to Pipeline Setup .","title":"How do I download the models?"},{"location":"faq.html#how-do-i-simulate-rtsp-cameras","text":"You can use the camera simulator script camera-simulator.sh to run simulated RTSP camera streams. For more details on the script, refer to Run Camera Simulator page.","title":"How do I simulate RTSP cameras?"},{"location":"faq.html#how-do-i-download-video-files-for-benchmarking","text":"You can download your own media file or use the the provided download_sample_videos.sh script to download an existing media file. For more details, refer to Benchmark Pipeline .","title":"How do I download video files for benchmarking?"},{"location":"faq.html#how-do-i-run-different-types-of-pipelines","text":"For details on running different types of pipelines, refer to Quick Start Guide .","title":"How do I run different types of pipelines?"},{"location":"faq.html#where-are-the-pipeline-container-logs-located","text":"Pipeline container logs are redirected to a text file at automated-self-checkout/results . For every new pipeline a new log file pipeline##.log (eg.: pipeline0.log, pipeline1.log, ... etc.) is created. Note As pipeline container logs are redirected to separate text files, docker logs displayed using portainer or command docker logs <**CONTAINER**> will be empty.","title":"Where are the pipeline container logs located ?"},{"location":"hardwaresetup.html","text":"System Setup Prerequisites To build the Intel\u00ae Automated Self-Checkout Reference Package, you need: Ubuntu LTS ( 22.04 or 20.04 ) Docker (Tested on version >= 23.0.0) Docker Compose v2 (Required, if using docker compose feature) Git Click the links for corresponding set up instructions. Hardware Dependent Installation 11th/12th Gen Intel\u00ae Core\u2122 Processor Intel\u00ae Xeon\u00ae Processor Intel\u00ae Data Center GPU Flex 140/170 Intel\u00ae Arc\u2122 Download Ubuntu 20.04 and follow these installation steps . Install Docker* Engine Note To avoid typing sudo when running the Docker command, follow these steps . [ Optional ] Install Docker Compose v2 , if using the docker compose feature Install Git Set up the pipeline Download Ubuntu 22.04 and follow these installation steps . Install Docker Engine [ Optional ] Install Docker Compose v2 , if using the docker compose feature Install Git Set up the pipeline Download Ubuntu 22.04 and follow these installation steps . Update the Kernel Warning After the kernel is updated, apt-get install might not work due to the unsupported kernel versions that were installed. To resolve this issue, do the following: Find all the installed kernels bash dpkg --list | grep -E -i --color 'linux-image|linux-headers' Then remove the unsupported kernels. The example below will remove the installed kernel 5.19: bash sudo apt-get purge -f 'linux--5.19' Install Docker Engine [ Optional ] Install Docker Compose v2 , if using the docker compose feature Install Git Set up the pipeline Download Ubuntu 20.04 and follow these installation steps . Update the Kernel Warning After the kernel is updated, apt-get install might not work due to the unsupported kernel versions that were installed. To resolve this issue, do the following: Find all the installed kernels bash dpkg --list | grep -E -i --color 'linux-image|linux-headers' Then remove the unsupported kernels. The example below will remove the installed kernel 5.19: bash sudo apt-get purge -f 'linux--5.19' Install Docker Engine [ Optional ] Install Docker Compose v2 , if using the docker compose feature Install Git Set up the pipeline","title":"System Setup"},{"location":"hardwaresetup.html#system-setup","text":"","title":"System Setup"},{"location":"hardwaresetup.html#prerequisites","text":"To build the Intel\u00ae Automated Self-Checkout Reference Package, you need: Ubuntu LTS ( 22.04 or 20.04 ) Docker (Tested on version >= 23.0.0) Docker Compose v2 (Required, if using docker compose feature) Git Click the links for corresponding set up instructions.","title":"Prerequisites"},{"location":"hardwaresetup.html#hardware-dependent-installation","text":"11th/12th Gen Intel\u00ae Core\u2122 Processor Intel\u00ae Xeon\u00ae Processor Intel\u00ae Data Center GPU Flex 140/170 Intel\u00ae Arc\u2122 Download Ubuntu 20.04 and follow these installation steps . Install Docker* Engine Note To avoid typing sudo when running the Docker command, follow these steps . [ Optional ] Install Docker Compose v2 , if using the docker compose feature Install Git Set up the pipeline Download Ubuntu 22.04 and follow these installation steps . Install Docker Engine [ Optional ] Install Docker Compose v2 , if using the docker compose feature Install Git Set up the pipeline Download Ubuntu 22.04 and follow these installation steps . Update the Kernel Warning After the kernel is updated, apt-get install might not work due to the unsupported kernel versions that were installed. To resolve this issue, do the following: Find all the installed kernels bash dpkg --list | grep -E -i --color 'linux-image|linux-headers' Then remove the unsupported kernels. The example below will remove the installed kernel 5.19: bash sudo apt-get purge -f 'linux--5.19' Install Docker Engine [ Optional ] Install Docker Compose v2 , if using the docker compose feature Install Git Set up the pipeline Download Ubuntu 20.04 and follow these installation steps . Update the Kernel Warning After the kernel is updated, apt-get install might not work due to the unsupported kernel versions that were installed. To resolve this issue, do the following: Find all the installed kernels bash dpkg --list | grep -E -i --color 'linux-image|linux-headers' Then remove the unsupported kernels. The example below will remove the installed kernel 5.19: bash sudo apt-get purge -f 'linux--5.19' Install Docker Engine [ Optional ] Install Docker Compose v2 , if using the docker compose feature Install Git Set up the pipeline","title":"Hardware Dependent Installation"},{"location":"platforms.html","text":"Supported Platforms Following are the list of supported platforms: CPU 11th Gen Intel\u00ae Core\u2122 i5 Processor/11th Gen Intel\u00ae Core\u2122 i7 Processor 12th Gen Intel\u00ae Core\u2122 i5 Processor/12th Gen Intel\u00ae Core\u2122 i7 Processor Intel\u00ae Xeon\u00ae Platinum 8351N Processor GPU Intel\u00ae Data Center GPU Flex 140 Intel\u00ae Data Center GPU Flex 170 Intel\u00ae Arc\u2122 A770M Graphics To set up the platform, refer to Hardware Setup .","title":"Supported Platforms"},{"location":"platforms.html#supported-platforms","text":"Following are the list of supported platforms:","title":"Supported Platforms"},{"location":"platforms.html#cpu","text":"11th Gen Intel\u00ae Core\u2122 i5 Processor/11th Gen Intel\u00ae Core\u2122 i7 Processor 12th Gen Intel\u00ae Core\u2122 i5 Processor/12th Gen Intel\u00ae Core\u2122 i7 Processor Intel\u00ae Xeon\u00ae Platinum 8351N Processor","title":"CPU"},{"location":"platforms.html#gpu","text":"Intel\u00ae Data Center GPU Flex 140 Intel\u00ae Data Center GPU Flex 170 Intel\u00ae Arc\u2122 A770M Graphics To set up the platform, refer to Hardware Setup .","title":"GPU"},{"location":"query_usb_camera.html","text":"Query USB Camera Make sure a USB camera is plugged into the system Install the necessary libraries bash sudo apt update sudo apt install v4l-utils -y List available video cameras bash ls -l /dev/vid* Note To get information about the development video ids, check the Execute a video, from the available list, for more information bash v4l2-ctl --list-formats-ext -d /dev/video0 Note Here is information on how to . Example Here is an example to run the pipeline with a USB camera on video0 for the core system: bash sudo ./run.sh --platform core --inputsrc /dev/video0","title":"Query USB Camera"},{"location":"query_usb_camera.html#query-usb-camera","text":"Make sure a USB camera is plugged into the system Install the necessary libraries bash sudo apt update sudo apt install v4l-utils -y List available video cameras bash ls -l /dev/vid* Note To get information about the development video ids, check the Execute a video, from the available list, for more information bash v4l2-ctl --list-formats-ext -d /dev/video0 Note Here is information on how to . Example Here is an example to run the pipeline with a USB camera on video0 for the core system: bash sudo ./run.sh --platform core --inputsrc /dev/video0","title":"Query USB Camera"},{"location":"releasenotes.html","text":"Releases Release v1.0.1 Release v1.5.0","title":"Releases"},{"location":"releasenotes.html#releases","text":"Release v1.0.1 Release v1.5.0","title":"Releases"},{"location":"roadmap.html","text":"Roadmap Roadmap Timeline Notes Now Computer Vision Pipeline A single service that processes a streaming video and runs inferencing across products placed on a self-checkout station. Benchmark Script A script that allows the end user to reproduce our benchmark results. The script provides capabilities to run benchmarks on partner models. Benchmark Results Detailed benchmark results across GPU, CPU and GPU hardware SKUs Context Automation is rapidly becoming the key transformational strategy for retailers. Age old problems such as inventory control, planogram compliance, store security, store operations, warehousing are areas that can benefit from automation that is predominantly driven by computer vision and AI. Whilst computer vision-based solutions exist for many of these problems, not many have moved beyond pilots and scaled across supermarkets due to various technical and business reasons. We are looking to address the use of computer vision across a multiple set of use cases in the retail space. The first of these use cases will be to help address the scalability of vision enabled self-checkout solutions. This will be followed by use cases such as Loss Prevention, AI assisted Shopping Carts, Autonomous Stores and many more in the future as the retail landscape evolves. The goal of this roadmap is to provide key ingredients and easy decision making to our partners on their journey to build and deploy these use cases at scale. High-level Focus Vision enabled use cases will need to address four fundamental areas to build and deploy solutions at scale. Camera Management Cameras are a critical piece of the infrastructure, providing both video streams and images to be used across these use cases. We will be focusing on providing a consistent mechanism of onboarding different types of cameras and managing its lifecycle. Computer Vision Pipeline Workload deployment options and choices of frameworks makes the vision pipeline complex. We will focus our breaking down the pipeline into its individual services and making it easier to deploy and run across distributed architecture. Hardware Recommendation There are many unknowns when moving from a pilot to production and especially during the scale phase of the project. For every use case, it is important to know exactly what infrastructure is required to deploy and scale the solution. We plan to remove all the guess work around what hardware is required to run these workloads. Deployment with ISVs The time taken to operationalize new AI-based software in a new environment is often long. Working with our partners, we would focus our efforts in reducing the time to production. Out-of-Scope There are several items that will not be considered as part of this reference implementation. This is not an exhaustive list, but these are core exclusions as they are not our differentiators: We will not build or recommend AI models We will not build end-to-end solutions We will not advocate for a specific software deployment architecture Next Distributed Architecture Separating out media pre-processing, AI inferencing and post-processing as completely independent services. Providing a mechanism to deploy and run the individual services across distributed heterogeneous compute. Publish all events to an Enterprise Service Bus (ESB). Update Benchmark script Update the script to reflect the new distributed architecture. Benchmark Results Benchmark results across CPU and GPU and GPU only hardware SKUs (previous results updated and new SKUs added) Later Model Drift and updates Monitor model accuracy and rectify model drift. Add new models in live systems. Edge Training Provide a mechanism to do localized instore training to ensure new products are identified without delay. Hierarchical model support Provide a mechanism to have a hierarchy of models that gets chosen and executed based on initial segmentation. Dev Cloud Support Provide a cloud environment for partners to access the hardware and run benchmarks. Camera Management Provide a mechanism to onboard and drive the camera lifecycle across the store. Disclaimer : The roadmap is for informational purposes only and is subject to change. Help Drive Our Roadmap Priorities We want to drive our roadmap by building the most valuable assets/ingredients to our partners. Our roadmap will be public and will provide complete transparency to ensure we receive continuous feedback from our partners. Raise any issues and requirements to help us guide our roadmap priorities, and in doing so we can drive the retail vertical forward. Open an issue on GitHub to report a problem or to provide feedback.","title":"Roadmap"},{"location":"roadmap.html#roadmap","text":"Roadmap Timeline Notes Now Computer Vision Pipeline A single service that processes a streaming video and runs inferencing across products placed on a self-checkout station. Benchmark Script A script that allows the end user to reproduce our benchmark results. The script provides capabilities to run benchmarks on partner models. Benchmark Results Detailed benchmark results across GPU, CPU and GPU hardware SKUs Context Automation is rapidly becoming the key transformational strategy for retailers. Age old problems such as inventory control, planogram compliance, store security, store operations, warehousing are areas that can benefit from automation that is predominantly driven by computer vision and AI. Whilst computer vision-based solutions exist for many of these problems, not many have moved beyond pilots and scaled across supermarkets due to various technical and business reasons. We are looking to address the use of computer vision across a multiple set of use cases in the retail space. The first of these use cases will be to help address the scalability of vision enabled self-checkout solutions. This will be followed by use cases such as Loss Prevention, AI assisted Shopping Carts, Autonomous Stores and many more in the future as the retail landscape evolves. The goal of this roadmap is to provide key ingredients and easy decision making to our partners on their journey to build and deploy these use cases at scale. High-level Focus Vision enabled use cases will need to address four fundamental areas to build and deploy solutions at scale. Camera Management Cameras are a critical piece of the infrastructure, providing both video streams and images to be used across these use cases. We will be focusing on providing a consistent mechanism of onboarding different types of cameras and managing its lifecycle. Computer Vision Pipeline Workload deployment options and choices of frameworks makes the vision pipeline complex. We will focus our breaking down the pipeline into its individual services and making it easier to deploy and run across distributed architecture. Hardware Recommendation There are many unknowns when moving from a pilot to production and especially during the scale phase of the project. For every use case, it is important to know exactly what infrastructure is required to deploy and scale the solution. We plan to remove all the guess work around what hardware is required to run these workloads. Deployment with ISVs The time taken to operationalize new AI-based software in a new environment is often long. Working with our partners, we would focus our efforts in reducing the time to production. Out-of-Scope There are several items that will not be considered as part of this reference implementation. This is not an exhaustive list, but these are core exclusions as they are not our differentiators: We will not build or recommend AI models We will not build end-to-end solutions We will not advocate for a specific software deployment architecture Next Distributed Architecture Separating out media pre-processing, AI inferencing and post-processing as completely independent services. Providing a mechanism to deploy and run the individual services across distributed heterogeneous compute. Publish all events to an Enterprise Service Bus (ESB). Update Benchmark script Update the script to reflect the new distributed architecture. Benchmark Results Benchmark results across CPU and GPU and GPU only hardware SKUs (previous results updated and new SKUs added) Later Model Drift and updates Monitor model accuracy and rectify model drift. Add new models in live systems. Edge Training Provide a mechanism to do localized instore training to ensure new products are identified without delay. Hierarchical model support Provide a mechanism to have a hierarchy of models that gets chosen and executed based on initial segmentation. Dev Cloud Support Provide a cloud environment for partners to access the hardware and run benchmarks. Camera Management Provide a mechanism to onboard and drive the camera lifecycle across the store. Disclaimer : The roadmap is for informational purposes only and is subject to change. Help Drive Our Roadmap Priorities We want to drive our roadmap by building the most valuable assets/ingredients to our partners. Our roadmap will be public and will provide complete transparency to ensure we receive continuous feedback from our partners. Raise any issues and requirements to help us guide our roadmap priorities, and in doing so we can drive the retail vertical forward. Open an issue on GitHub to report a problem or to provide feedback.","title":"Roadmap"},{"location":"troubleshooting.html","text":"Troubleshooting Q: Why is the performance sometimes on CPU better than on GPU, when running pipeline benchmarking like stream density ? A: The performance of pipeline benchmarking strongly depends on the models. Specifically for yolov5s object detection, it is recommended to use the model precision FP32 when it is running on device GPU . If supported, then you can change the model precision by going to the folder configs/opencv-ovms/models/2022 from the root of project folder and editing the base_path for that particular model in the config_template.json file. For example, you can change the the base_path of FP16 to FP32 assuming the precision FP32 of the model yolov5s is available: json ... \"config\": { \"name\": \"yolov5s\", \"base_path\": \"/models/yolov5s/FP32-INT8\", \"layout\": \"NHWC:NCHW\", ... }","title":"Troubleshooting"},{"location":"troubleshooting.html#troubleshooting","text":"Q: Why is the performance sometimes on CPU better than on GPU, when running pipeline benchmarking like stream density ? A: The performance of pipeline benchmarking strongly depends on the models. Specifically for yolov5s object detection, it is recommended to use the model precision FP32 when it is running on device GPU . If supported, then you can change the model precision by going to the folder configs/opencv-ovms/models/2022 from the root of project folder and editing the base_path for that particular model in the config_template.json file. For example, you can change the the base_path of FP16 to FP32 assuming the precision FP32 of the model yolov5s is available: json ... \"config\": { \"name\": \"yolov5s\", \"base_path\": \"/models/yolov5s/FP32-INT8\", \"layout\": \"NHWC:NCHW\", ... }","title":"Troubleshooting"},{"location":"webcam_rtsp.html","text":"Webcam to RTSP When using /dev/video0 as input, only one container can use the webcam at the time. If the intention is to run multiple pipelines at once using a webcam as the input, then, the solution is to covert the webcam to an RTSP path. Run: bash make webcam-rtsp Then use the path rtsp://127.0.0.1:8554/cam as input","title":"Webcam to RTSP"},{"location":"webcam_rtsp.html#webcam-to-rtsp","text":"When using /dev/video0 as input, only one container can use the webcam at the time. If the intention is to run multiple pipelines at once using a webcam as the input, then, the solution is to covert the webcam to an RTSP path. Run: bash make webcam-rtsp Then use the path rtsp://127.0.0.1:8554/cam as input","title":"Webcam to RTSP"},{"location":"OVMS/camera_serial_number.html","text":"Get Serial Number of Intel\u00ae RealSense\u2122 Camera Do the following to get the serial number of an Intel\u00ae RealSense\u2122 Camera: Build the RealSense version of dlstreamer Docker image if not done yet: bash make build-dlstreamer-realsense Plug in your Intel\u00ae RealSense\u2122 Camera into the system; Use the makefile target get-realsense-serial-num to get the serial number of your Intel\u00ae RealSense\u2122 Camera: bash $ make get-realsense-serial-num You should see a serial number printed out. If you do not see the expected results, check if the Intel\u00ae RealSense\u2122 Camera is plugged in.","title":"Supporting RealSense Camera"},{"location":"OVMS/camera_serial_number.html#get-serial-number-of-intel-realsensetm-camera","text":"Do the following to get the serial number of an Intel\u00ae RealSense\u2122 Camera: Build the RealSense version of dlstreamer Docker image if not done yet: bash make build-dlstreamer-realsense Plug in your Intel\u00ae RealSense\u2122 Camera into the system; Use the makefile target get-realsense-serial-num to get the serial number of your Intel\u00ae RealSense\u2122 Camera: bash $ make get-realsense-serial-num You should see a serial number printed out. If you do not see the expected results, check if the Intel\u00ae RealSense\u2122 Camera is plugged in.","title":"Get Serial Number of Intel\u00ae RealSense\u2122 Camera"},{"location":"OVMS/capiPipelineRun.html","text":"OpenVINO OVMS C-API Pipeline Run OpenVINO Model Server has many ways to run inferencing pipeline : TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here is a demonstration for using OVMS C API method to run face detection inferencing pipeline with steps below: Add new section to model configuration file for model server Add pipeline specific files Add environment variable file dependency Add a profile launcher pipeline configuration file Build and run Add New Section To Model Config File for Model Server Here is the template config file location: configs/opencv-ovms/models/2022/config_template.json , edit the file and append the following face detection model configuration section to the template json , {\"config\": { \"name\": \"face-detection-retail-0005\", \"base_path\": \"face-detection-retail-0005/FP16-INT8\", \"shape\": \"(1,3,800,800)\", \"nireq\": 2, \"batch_size\":\"1\", \"plugin_config\": {\"PERFORMANCE_HINT\": \"LATENCY\"}, \"target_device\": \"{target_device}\"}, \"latest\": { \"num_versions\": 2 } } Note shape is optional and takes precedence over batch_size, please remove this attribute if you don't know the value for the model. Note Please leave target_device value as it is, as the value {target_device} will be recognized and replaced by script run. You can find the parameter description in the ovms docs . Add pipeline specific files Here is the list of files we added in directory of configs/opencv-ovms/gst_capi/pipelines/face_detection/ : main.cpp - this is all the work about pre-processing before sending to OVMS for inferencing and post-processing for displaying. Makefile - to help building the pre-processing and post-processing binary. Add Environment Variable File You can add multiple environment variable files to configs/opencv-ovms/envs/ directory for your pipeline. For face detection pipeline run, we have added configs/opencv-ovms/envs/capi_face_detection.env environment variable file. Below is a list of explanation for all environment variables and current default values we set for face detection pipeline run, this list can be extended for any future modification. EV Name Face Detection Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU & GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/face_detection/face_detection pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.9 detection threshold value in floating point that needs to be between 0.0 to 1.0 details of face detection pipeline environment variable file can be viewed in configs/opencv-ovms/envs/capi_face_detection.env . Add A Profile Launcher Configuration File The details about Profile Launcher configuration can be found here , details of capi face detection profile launcher configuration can be viewed in configs/opencv-ovms/cmd_client/res/capi_face_detection/configuration.yaml . Build and Run Here are the quick start steps to build and run OVMS C API face detection pipeline profile: Build gst-capi ovms with profile-launcher: make build-capi_face_detection Download sample video files: cd benchmark-scripts/ && ./download_sample_videos.sh && cd .. Start simulator camera: make run-camera-simulator To start face detection pipeline: PIPELINE_PROFILE=\"capi_face_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 Note The pipeline run will automatically download the OpenVINO model files listed in configs/opencv-ovms/models/2022/config_template.json","title":"C-API Face Detection Pipeline"},{"location":"OVMS/capiPipelineRun.html#openvino-ovms-c-api-pipeline-run","text":"OpenVINO Model Server has many ways to run inferencing pipeline : TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here is a demonstration for using OVMS C API method to run face detection inferencing pipeline with steps below: Add new section to model configuration file for model server Add pipeline specific files Add environment variable file dependency Add a profile launcher pipeline configuration file Build and run","title":"OpenVINO OVMS C-API Pipeline Run"},{"location":"OVMS/capiPipelineRun.html#add-new-section-to-model-config-file-for-model-server","text":"Here is the template config file location: configs/opencv-ovms/models/2022/config_template.json , edit the file and append the following face detection model configuration section to the template json , {\"config\": { \"name\": \"face-detection-retail-0005\", \"base_path\": \"face-detection-retail-0005/FP16-INT8\", \"shape\": \"(1,3,800,800)\", \"nireq\": 2, \"batch_size\":\"1\", \"plugin_config\": {\"PERFORMANCE_HINT\": \"LATENCY\"}, \"target_device\": \"{target_device}\"}, \"latest\": { \"num_versions\": 2 } } Note shape is optional and takes precedence over batch_size, please remove this attribute if you don't know the value for the model. Note Please leave target_device value as it is, as the value {target_device} will be recognized and replaced by script run. You can find the parameter description in the ovms docs .","title":"Add New Section To Model Config File for Model Server"},{"location":"OVMS/capiPipelineRun.html#add-pipeline-specific-files","text":"Here is the list of files we added in directory of configs/opencv-ovms/gst_capi/pipelines/face_detection/ : main.cpp - this is all the work about pre-processing before sending to OVMS for inferencing and post-processing for displaying. Makefile - to help building the pre-processing and post-processing binary.","title":"Add pipeline specific files"},{"location":"OVMS/capiPipelineRun.html#add-environment-variable-file","text":"You can add multiple environment variable files to configs/opencv-ovms/envs/ directory for your pipeline. For face detection pipeline run, we have added configs/opencv-ovms/envs/capi_face_detection.env environment variable file. Below is a list of explanation for all environment variables and current default values we set for face detection pipeline run, this list can be extended for any future modification. EV Name Face Detection Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU & GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/face_detection/face_detection pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.9 detection threshold value in floating point that needs to be between 0.0 to 1.0 details of face detection pipeline environment variable file can be viewed in configs/opencv-ovms/envs/capi_face_detection.env .","title":"Add Environment Variable File"},{"location":"OVMS/capiPipelineRun.html#add-a-profile-launcher-configuration-file","text":"The details about Profile Launcher configuration can be found here , details of capi face detection profile launcher configuration can be viewed in configs/opencv-ovms/cmd_client/res/capi_face_detection/configuration.yaml .","title":"Add A Profile Launcher Configuration File"},{"location":"OVMS/capiPipelineRun.html#build-and-run","text":"Here are the quick start steps to build and run OVMS C API face detection pipeline profile: Build gst-capi ovms with profile-launcher: make build-capi_face_detection Download sample video files: cd benchmark-scripts/ && ./download_sample_videos.sh && cd .. Start simulator camera: make run-camera-simulator To start face detection pipeline: PIPELINE_PROFILE=\"capi_face_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 Note The pipeline run will automatically download the OpenVINO model files listed in configs/opencv-ovms/models/2022/config_template.json","title":"Build and Run"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html","text":"OpenVINO OVMS C-API Yolov5 Ensemble Pipeline Run OpenVINO Model Server has many ways to run inferencing pipeline : TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here we are demonstrating for using OVMS C API method to run inferencing pipeline yolov5s ensemble models in following steps: Add new section to model configuration file for model server Add pipeline specific files Add environment variable file dependency Add a profile launcher pipeline configuration file Build and run Clean up Add New Section To Model Config File for Model Server The model template configuration file has been updated with model configs of yolov5, efficientnetb0_FP32INT8 and custom configurations, please view configs/opencv-ovms/models/2022/config_template.json for detail. Note New model yolov5 is similar to yolov5s configuration except the layout difference. Add pipeline specific files The pre-processing and post-processing work files are added in directory of configs/opencv-ovms/gst_capi/pipelines/capi_yolov5_ensemble/ , please view directory for details. Add Environment Variable File You can add multiple environment variable files to configs/opencv-ovms/envs/ directory for your pipeline, we've added capi_yolov5_ensemble.env for yolov5 ensemble pipeline run. Below is a list of explanation for all environment variables and current default values we set, this list can be extended for any future modification. EV Name Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU & GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/capi_yolov5_ensemble/capi_yolov5_ensemble pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.7 detection threshold value in floating point that needs to be between 0.0 to 1.0 BARCODE 1 For capi_yolov5_ensemble pipeline, you can enable barcode detection. value: 0 or 1 details of yolov5s pipeline environment variable file can be viewed in configs/opencv-ovms/envs/capi_yolov5_ensemble.env . Add A Profile Launcher Configuration File The details about Profile Launcher configuration can be found here , details for yolov5 pipeline profile launcher configuration can be viewed in configs/opencv-ovms/cmd_client/res/capi_yolov5_ensemble/configuration.yaml Build and Run Here are the quick start steps to build and run capi yolov5 pipeline profile : Build docker image with profile-launcher: make build-capi_yolov5_ensemble Download sample video files: cd benchmark-scripts/ && ./download_sample_videos.sh && cd .. Start simulator camera: make run-camera-simulator To start the pipeline run: PIPELINE_PROFILE=\"capi_yolov5_ensemble\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 Note The pipeline will automatically download the OpenVINO model files listed in configs/opencv-ovms/models/2022/config_template.json Clean Up To stop existing container: make clean-capi_yolov5_ensemble To stop all running containers including camera simulator and remove all log files: make clean-all","title":"OpenVINO OVMS C-API Yolov5 Ensemble Pipeline Run"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#openvino-ovms-c-api-yolov5-ensemble-pipeline-run","text":"OpenVINO Model Server has many ways to run inferencing pipeline : TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here we are demonstrating for using OVMS C API method to run inferencing pipeline yolov5s ensemble models in following steps: Add new section to model configuration file for model server Add pipeline specific files Add environment variable file dependency Add a profile launcher pipeline configuration file Build and run Clean up","title":"OpenVINO OVMS C-API Yolov5 Ensemble Pipeline Run"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-new-section-to-model-config-file-for-model-server","text":"The model template configuration file has been updated with model configs of yolov5, efficientnetb0_FP32INT8 and custom configurations, please view configs/opencv-ovms/models/2022/config_template.json for detail. Note New model yolov5 is similar to yolov5s configuration except the layout difference.","title":"Add New Section To Model Config File for Model Server"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-pipeline-specific-files","text":"The pre-processing and post-processing work files are added in directory of configs/opencv-ovms/gst_capi/pipelines/capi_yolov5_ensemble/ , please view directory for details.","title":"Add pipeline specific files"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-environment-variable-file","text":"You can add multiple environment variable files to configs/opencv-ovms/envs/ directory for your pipeline, we've added capi_yolov5_ensemble.env for yolov5 ensemble pipeline run. Below is a list of explanation for all environment variables and current default values we set, this list can be extended for any future modification. EV Name Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU & GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/capi_yolov5_ensemble/capi_yolov5_ensemble pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.7 detection threshold value in floating point that needs to be between 0.0 to 1.0 BARCODE 1 For capi_yolov5_ensemble pipeline, you can enable barcode detection. value: 0 or 1 details of yolov5s pipeline environment variable file can be viewed in configs/opencv-ovms/envs/capi_yolov5_ensemble.env .","title":"Add Environment Variable File"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-a-profile-launcher-configuration-file","text":"The details about Profile Launcher configuration can be found here , details for yolov5 pipeline profile launcher configuration can be viewed in configs/opencv-ovms/cmd_client/res/capi_yolov5_ensemble/configuration.yaml","title":"Add A Profile Launcher Configuration File"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#build-and-run","text":"Here are the quick start steps to build and run capi yolov5 pipeline profile : Build docker image with profile-launcher: make build-capi_yolov5_ensemble Download sample video files: cd benchmark-scripts/ && ./download_sample_videos.sh && cd .. Start simulator camera: make run-camera-simulator To start the pipeline run: PIPELINE_PROFILE=\"capi_yolov5_ensemble\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 Note The pipeline will automatically download the OpenVINO model files listed in configs/opencv-ovms/models/2022/config_template.json","title":"Build and Run"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#clean-up","text":"To stop existing container: make clean-capi_yolov5_ensemble To stop all running containers including camera simulator and remove all log files: make clean-all","title":"Clean Up"},{"location":"OVMS/capiYolov5PipelineRun.html","text":"OpenVINO OVMS C-API Yolov5 Pipeline Run OpenVINO Model Server has many ways to run inferencing pipeline : TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here we are demonstrating for using OVMS C API method to run inferencing pipeline yolov5s model in following steps: Add new section to model configuration file for model server Add pipeline specific files Add environment variable file dependency Add a profile launcher pipeline configuration file Build and run Add New Section To Model Config File for Model Server Here is the template config file location: configs/opencv-ovms/models/2022/config_template.json , edit the file and append the new model's configuration into the template, such as yolov5 model as shown below: json { \"config\": { \"name\": \"yolov5s\", \"base_path\": \"/models/yolov5s/FP16-INT8\", \"layout\": \"NHWC:NCHW\", \"shape\": \"(1,416,416,3)\", \"nireq\": 1, \"batch_size\": \"1\", \"plugin_config\": { \"PERFORMANCE_HINT\": \"LATENCY\" }, \"target_device\": \"{target_device}\" } } Note shape is optional and takes precedence over batch_size, please remove this attribute if you don't know the value for the model. Note Please leave target_device value as it is, as the value {target_device} will be recognized and replaced by script run. You can find the parameter description in the ovms docs . Add pipeline specific files Here is the list of files we added in directory of configs/opencv-ovms/gst_capi/pipelines/capi_yolov5/ : main.cpp - this is all the work about pre-processing before sending to OVMS for inferencing and post-processing for displaying. Makefile - to help building the pre-processing and post-processing binary. Add Environment Variable File You can add multiple environment variable files to configs/opencv-ovms/envs/ directory for your pipeline, we've added capi_yolov5.env for yolov5 pipeline run. Below is a list of explanation for all environment variables and current default values we set, this list can be extended for any future modification. EV Name Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU & GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/capi_yolov5/capi_yolov5 pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.7 detection threshold value in floating point that needs to be between 0.0 to 1.0 details of yolov5s pipeline environment variable file can be viewed in configs/opencv-ovms/envs/capi_yolov5.env . Add A Profile Launcher Configuration File The details about Profile Launcher configuration can be found here , details for yolov5 pipeline profile launcher configuration can be viewed in configs/opencv-ovms/cmd_client/res/capi_yolov5/configuration.yaml Build and Run Here are the quick start steps to build and run capi yolov5 pipeline profile : Build docker image with profile-launcher: make build-capi_yolov5 Download sample video files: cd benchmark-scripts/ && ./download_sample_videos.sh && cd .. Start simulator camera: make run-camera-simulator To start the pipeline run: PIPELINE_PROFILE=\"capi_yolov5\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1 Note The pipeline will automatically download the OpenVINO model files listed in configs/opencv-ovms/models/2022/config_template.json","title":"OpenVINO OVMS C-API Yolov5 Pipeline Run"},{"location":"OVMS/capiYolov5PipelineRun.html#openvino-ovms-c-api-yolov5-pipeline-run","text":"OpenVINO Model Server has many ways to run inferencing pipeline : TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here we are demonstrating for using OVMS C API method to run inferencing pipeline yolov5s model in following steps: Add new section to model configuration file for model server Add pipeline specific files Add environment variable file dependency Add a profile launcher pipeline configuration file Build and run","title":"OpenVINO OVMS C-API Yolov5 Pipeline Run"},{"location":"OVMS/capiYolov5PipelineRun.html#add-new-section-to-model-config-file-for-model-server","text":"Here is the template config file location: configs/opencv-ovms/models/2022/config_template.json , edit the file and append the new model's configuration into the template, such as yolov5 model as shown below: json { \"config\": { \"name\": \"yolov5s\", \"base_path\": \"/models/yolov5s/FP16-INT8\", \"layout\": \"NHWC:NCHW\", \"shape\": \"(1,416,416,3)\", \"nireq\": 1, \"batch_size\": \"1\", \"plugin_config\": { \"PERFORMANCE_HINT\": \"LATENCY\" }, \"target_device\": \"{target_device}\" } } Note shape is optional and takes precedence over batch_size, please remove this attribute if you don't know the value for the model. Note Please leave target_device value as it is, as the value {target_device} will be recognized and replaced by script run. You can find the parameter description in the ovms docs .","title":"Add New Section To Model Config File for Model Server"},{"location":"OVMS/capiYolov5PipelineRun.html#add-pipeline-specific-files","text":"Here is the list of files we added in directory of configs/opencv-ovms/gst_capi/pipelines/capi_yolov5/ : main.cpp - this is all the work about pre-processing before sending to OVMS for inferencing and post-processing for displaying. Makefile - to help building the pre-processing and post-processing binary.","title":"Add pipeline specific files"},{"location":"OVMS/capiYolov5PipelineRun.html#add-environment-variable-file","text":"You can add multiple environment variable files to configs/opencv-ovms/envs/ directory for your pipeline, we've added capi_yolov5.env for yolov5 pipeline run. Below is a list of explanation for all environment variables and current default values we set, this list can be extended for any future modification. EV Name Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU & GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/capi_yolov5/capi_yolov5 pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.7 detection threshold value in floating point that needs to be between 0.0 to 1.0 details of yolov5s pipeline environment variable file can be viewed in configs/opencv-ovms/envs/capi_yolov5.env .","title":"Add Environment Variable File"},{"location":"OVMS/capiYolov5PipelineRun.html#add-a-profile-launcher-configuration-file","text":"The details about Profile Launcher configuration can be found here , details for yolov5 pipeline profile launcher configuration can be viewed in configs/opencv-ovms/cmd_client/res/capi_yolov5/configuration.yaml","title":"Add A Profile Launcher Configuration File"},{"location":"OVMS/capiYolov5PipelineRun.html#build-and-run","text":"Here are the quick start steps to build and run capi yolov5 pipeline profile : Build docker image with profile-launcher: make build-capi_yolov5 Download sample video files: cd benchmark-scripts/ && ./download_sample_videos.sh && cd .. Start simulator camera: make run-camera-simulator To start the pipeline run: PIPELINE_PROFILE=\"capi_yolov5\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1 Note The pipeline will automatically download the OpenVINO model files listed in configs/opencv-ovms/models/2022/config_template.json","title":"Build and Run"},{"location":"OVMS/pipelineDockerCompose.html","text":"Docker-Compose for Developer Toolbox Pipelines can be run using docker-compose files. Changes are custom made inside the docker-compose.yml file for integration with the Developer Toolbox. Note To utilize all the features offered by Automated Self-Checkout, run the pipelines as illustrated in the section Run Pipelines . Steps to Run Pipelines Prerequisites Before running, Set Up the Pipelines . Note Ensure Docker Compose v2 is installed in order to run the pipelines via this feature. Customize the docker-compose.yml to add the number of camera simulators required and the number of different type of pipelines that need to be run Note Follow all the instructions in docker-compose.yml for customizations. Run the pipelines bash make run-pipelines All the containers i.e camera simulators, OVMS server and pipelines should start without any errors in portainer as shown below in Figure 1 - Figure 1: Pipelines Running Successfully Stop the pipelines bash make down-pipelines","title":"Run Pipelines with Docker-Compose for Developer Toolbox"},{"location":"OVMS/pipelineDockerCompose.html#docker-compose-for-developer-toolbox","text":"Pipelines can be run using docker-compose files. Changes are custom made inside the docker-compose.yml file for integration with the Developer Toolbox. Note To utilize all the features offered by Automated Self-Checkout, run the pipelines as illustrated in the section Run Pipelines .","title":"Docker-Compose for Developer Toolbox"},{"location":"OVMS/pipelineDockerCompose.html#steps-to-run-pipelines","text":"Prerequisites Before running, Set Up the Pipelines . Note Ensure Docker Compose v2 is installed in order to run the pipelines via this feature. Customize the docker-compose.yml to add the number of camera simulators required and the number of different type of pipelines that need to be run Note Follow all the instructions in docker-compose.yml for customizations. Run the pipelines bash make run-pipelines All the containers i.e camera simulators, OVMS server and pipelines should start without any errors in portainer as shown below in Figure 1 - Figure 1: Pipelines Running Successfully Stop the pipelines bash make down-pipelines","title":"Steps to Run Pipelines"},{"location":"OVMS/pipelinebenchmarking.html","text":"Computer Vision Pipeline Benchmarking You can benchmark pipelines with a collection of scripts to get the pipeline performance metrics such as video processing in frame-per-second (FPS), memory usage, power consumption, and so on. Prerequisites Before benchmarking, make sure you set up the pipeline . Steps to Benchmark Computer Vision Pipelines Build the benchmark Docker* images Benchmark scripts are containerized inside Docker. The following table lists the commands for various platforms. Choose and run the command corresponding to your hardware configuration. Platform Docker Build Command Check Success Intel\u00ae integrated and Arc\u2122 GPUs cd benchmark-scripts make build-benchmark make build-igt Docker images command to show both benchmark:dev and benchmark:igt images Intel\u00ae Flex GPUs cd benchmark-scripts make build-benchmark make build-xpu Docker images command to show both benchmark:dev and benchmark:xpu images Warning Build command may take a while, depending on your internet connection and machine specifications. Determine the appropriate parameters for Input source type Platform Pipeline Profile Choose a given pipeline profile, and run the benchmark for that pipeline profile. To see all available pipeline profiles, use make list-profiles command on the project base directory. ```console if you are in the benchmark-scripts directory then do a cd .. $ cd .. $ make list-profiles ``` The benchmark.sh shell script is in the base / benchmark_scripts directory. The following snippet give an example to run multiple pipelines benchmarking for the object detection pipelines. bash cd ./benchmark_scripts PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --pipelines <number of pipelines> --logdir <output dir>/data --init_duration 30 --duration 120 --platform <core|xeon|dgpu.x> --inputsrc <ex:4k rtsp stream with 10 objects> Note The benchmark.sh can either benchmark a specific number of pipelines or benchmark stream density based on the desired FPS. Input Source Type The benchmark script can take either of the following video input sources: Real Time Streaming Protocol (RTSP) bash --inputsrc rtsp://127.0.0.1:8554/camera_0 Note Using RTSP source with benchmark.sh will automatically run the camera simulator. The camera simulator will start an RTSP stream for each video file in the sample-media folder. USB Camera bash --inputsrc /dev/video<N>, where N is 0 or an integer Intel\u00ae RealSense\u2122 Camera bash --inputsrc <RealSense camera serial number> To know the serial number of the Intel\u00ae RealSense\u2122 Camera, refer to Get Serial Number of Intel\u00ae RealSense\u2122 Camera . Video File bash --inputsrc file:my_video_file.mp4 Note Video files must be in the sample-media folder, so that the Docker container can access the files. You can provide your own video files or download a sample video file using the script download_sample_videos.sh . Platform Intel\u00ae Core\u2122 Processor --platform core.x if GPUs are available, then replace this parameter with targeted GPUs such as core (for all GPUs), core.0, core.1, and so on --platform core will evenly distribute and utilize all available core GPUs Intel\u00ae Xeon\u00ae Scalable Processor --platform xeon will use the Xeon CPU for the pipelines DGPU (Intel\u00ae Data Center GPU Flex 140, Intel\u00ae Data Center GPU Flex 170, and Intel\u00ae Arc\u2122 Setup) --platform dgpu.x replace this parameter with targeted GPUs such as dgpu (for all GPUs), dgpu.0, dgpu.1, and so on --platform dgpu will evenly distribute and utilize all available dgpus Benchmark Specified Number of Pipelines The primary purpose of benchmarking with a specified number of pipelines is to discover the performance and system requirements for a given use case. Example Here is an example of running benchmarking object detection pipelines with specified number of pipelines: bash PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --pipelines <number of pipelines> --logdir <output dir>/data --init_duration 30 --duration 120 --platform <core|xeon|dgpu.x> --inputsrc <ex:4k rtsp stream with 10 objects> where, the configurable input parameters include: --performance_mode configures the scaling governor of the system. Supported modes are performance and powersave (default). --logdir configures the benchmarking output directory --duration configures the duration, in number of seconds, the benchmarking will run --init_duration configures the duration, in number of seconds, to wait for system initialization before the benchmarking metrics or data collection begins Example You can run multiple pipeline benchmarking with different configurations before consolidating all pipeline output results. To get the consolidated pipeline results, run the following make command: bash make consolidate ROOT_DIRECTORY=<output dir> This command will consolidate the performance metrics that exist in the specified ROOT_DIRECTORY . Here is an example of consolidated output: Success Output of Consolidate_multiple_run_of_metrics.py excel ,Metric,data 0,Total Text count,0 1,Total Barcode count,2 2,Camera_1 FPS,15.0 3,Camera_0 FPS,15.0 4,CPU Utilization %,16.548 5,Memory Utilization %,21.162 6,Disk Read MB/s,0.0 7,Disk Write MB/s,0.025 8,S0 Memory Bandwidth Usage MB/s,1872.632 9,S0 Power Draw W,27.502 10,GPU_0 VDBOX0 Utilization %,0.0 11,GPU_0 GPU Utilization %,17.282 Benchmark Specified Profile There are several pipeline profiles that support different programming languages and different pipeline models. You may specify language choice and model input . Then you may prefix benchmark script run command with specific profile. An example of stream density benchmark script in golang: bash PIPELINE_PROFILE=\"grpc_go\" sudo -E ./benchmark.sh --stream_density 14.9 --logdir mytest/data --duration 60 --init_duration 20 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 An example of stream density benchmark script in python: bash PIPELINE_PROFILE=\"grpc_python\" sudo -E ./benchmark.sh --stream_density 14.9 --logdir mytest/data --duration 60 --init_duration 60 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 If prefix is not provided, then the default value is \"grpc_python\". Appendix: Benchmark Helper Scripts camera-simulator.sh : This script starts the camera simulator. Create two folders named camera-simulator and sample-media . Place camera-simulator.sh in the camera-simulator folder. Manually copy the video files to the sample-media folder or run the download_sample_videos.sh script to download sample videos. The camera-simulator.sh script will start a simulator for each .mp4 video that it finds in the sample-media folder and will enumerate them as camera_0, camera_1, and so on. Make sure that the path to the camera-simulator.sh script is mentioned correctly in the camera-simulator.sh script. stop_server.sh : This script stops and removes all Docker containers closing the pipelines.","title":"Quick Start Benchmarking"},{"location":"OVMS/pipelinebenchmarking.html#computer-vision-pipeline-benchmarking","text":"You can benchmark pipelines with a collection of scripts to get the pipeline performance metrics such as video processing in frame-per-second (FPS), memory usage, power consumption, and so on.","title":"Computer Vision Pipeline Benchmarking"},{"location":"OVMS/pipelinebenchmarking.html#prerequisites","text":"Before benchmarking, make sure you set up the pipeline .","title":"Prerequisites"},{"location":"OVMS/pipelinebenchmarking.html#steps-to-benchmark-computer-vision-pipelines","text":"Build the benchmark Docker* images Benchmark scripts are containerized inside Docker. The following table lists the commands for various platforms. Choose and run the command corresponding to your hardware configuration. Platform Docker Build Command Check Success Intel\u00ae integrated and Arc\u2122 GPUs cd benchmark-scripts make build-benchmark make build-igt Docker images command to show both benchmark:dev and benchmark:igt images Intel\u00ae Flex GPUs cd benchmark-scripts make build-benchmark make build-xpu Docker images command to show both benchmark:dev and benchmark:xpu images Warning Build command may take a while, depending on your internet connection and machine specifications. Determine the appropriate parameters for Input source type Platform Pipeline Profile Choose a given pipeline profile, and run the benchmark for that pipeline profile. To see all available pipeline profiles, use make list-profiles command on the project base directory. ```console","title":"Steps to Benchmark Computer Vision Pipelines"},{"location":"OVMS/pipelinebenchmarking.html#if-you-are-in-the-benchmark-scripts-directory-then-do-a-cd","text":"$ cd .. $ make list-profiles ``` The benchmark.sh shell script is in the base / benchmark_scripts directory. The following snippet give an example to run multiple pipelines benchmarking for the object detection pipelines. bash cd ./benchmark_scripts PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --pipelines <number of pipelines> --logdir <output dir>/data --init_duration 30 --duration 120 --platform <core|xeon|dgpu.x> --inputsrc <ex:4k rtsp stream with 10 objects> Note The benchmark.sh can either benchmark a specific number of pipelines or benchmark stream density based on the desired FPS.","title":"if you are in the benchmark-scripts directory then do a cd .."},{"location":"OVMS/pipelinebenchmarking.html#input-source-type","text":"The benchmark script can take either of the following video input sources: Real Time Streaming Protocol (RTSP) bash --inputsrc rtsp://127.0.0.1:8554/camera_0 Note Using RTSP source with benchmark.sh will automatically run the camera simulator. The camera simulator will start an RTSP stream for each video file in the sample-media folder. USB Camera bash --inputsrc /dev/video<N>, where N is 0 or an integer Intel\u00ae RealSense\u2122 Camera bash --inputsrc <RealSense camera serial number> To know the serial number of the Intel\u00ae RealSense\u2122 Camera, refer to Get Serial Number of Intel\u00ae RealSense\u2122 Camera . Video File bash --inputsrc file:my_video_file.mp4 Note Video files must be in the sample-media folder, so that the Docker container can access the files. You can provide your own video files or download a sample video file using the script download_sample_videos.sh .","title":"Input Source Type"},{"location":"OVMS/pipelinebenchmarking.html#platform","text":"Intel\u00ae Core\u2122 Processor --platform core.x if GPUs are available, then replace this parameter with targeted GPUs such as core (for all GPUs), core.0, core.1, and so on --platform core will evenly distribute and utilize all available core GPUs Intel\u00ae Xeon\u00ae Scalable Processor --platform xeon will use the Xeon CPU for the pipelines DGPU (Intel\u00ae Data Center GPU Flex 140, Intel\u00ae Data Center GPU Flex 170, and Intel\u00ae Arc\u2122 Setup) --platform dgpu.x replace this parameter with targeted GPUs such as dgpu (for all GPUs), dgpu.0, dgpu.1, and so on --platform dgpu will evenly distribute and utilize all available dgpus","title":"Platform"},{"location":"OVMS/pipelinebenchmarking.html#benchmark-specified-number-of-pipelines","text":"The primary purpose of benchmarking with a specified number of pipelines is to discover the performance and system requirements for a given use case. Example Here is an example of running benchmarking object detection pipelines with specified number of pipelines: bash PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --pipelines <number of pipelines> --logdir <output dir>/data --init_duration 30 --duration 120 --platform <core|xeon|dgpu.x> --inputsrc <ex:4k rtsp stream with 10 objects> where, the configurable input parameters include: --performance_mode configures the scaling governor of the system. Supported modes are performance and powersave (default). --logdir configures the benchmarking output directory --duration configures the duration, in number of seconds, the benchmarking will run --init_duration configures the duration, in number of seconds, to wait for system initialization before the benchmarking metrics or data collection begins Example You can run multiple pipeline benchmarking with different configurations before consolidating all pipeline output results. To get the consolidated pipeline results, run the following make command: bash make consolidate ROOT_DIRECTORY=<output dir> This command will consolidate the performance metrics that exist in the specified ROOT_DIRECTORY . Here is an example of consolidated output: Success Output of Consolidate_multiple_run_of_metrics.py excel ,Metric,data 0,Total Text count,0 1,Total Barcode count,2 2,Camera_1 FPS,15.0 3,Camera_0 FPS,15.0 4,CPU Utilization %,16.548 5,Memory Utilization %,21.162 6,Disk Read MB/s,0.0 7,Disk Write MB/s,0.025 8,S0 Memory Bandwidth Usage MB/s,1872.632 9,S0 Power Draw W,27.502 10,GPU_0 VDBOX0 Utilization %,0.0 11,GPU_0 GPU Utilization %,17.282","title":"Benchmark Specified Number of Pipelines"},{"location":"OVMS/pipelinebenchmarking.html#benchmark-specified-profile","text":"There are several pipeline profiles that support different programming languages and different pipeline models. You may specify language choice and model input . Then you may prefix benchmark script run command with specific profile. An example of stream density benchmark script in golang: bash PIPELINE_PROFILE=\"grpc_go\" sudo -E ./benchmark.sh --stream_density 14.9 --logdir mytest/data --duration 60 --init_duration 20 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 An example of stream density benchmark script in python: bash PIPELINE_PROFILE=\"grpc_python\" sudo -E ./benchmark.sh --stream_density 14.9 --logdir mytest/data --duration 60 --init_duration 60 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 If prefix is not provided, then the default value is \"grpc_python\".","title":"Benchmark Specified Profile"},{"location":"OVMS/pipelinebenchmarking.html#appendix-benchmark-helper-scripts","text":"camera-simulator.sh : This script starts the camera simulator. Create two folders named camera-simulator and sample-media . Place camera-simulator.sh in the camera-simulator folder. Manually copy the video files to the sample-media folder or run the download_sample_videos.sh script to download sample videos. The camera-simulator.sh script will start a simulator for each .mp4 video that it finds in the sample-media folder and will enumerate them as camera_0, camera_1, and so on. Make sure that the path to the camera-simulator.sh script is mentioned correctly in the camera-simulator.sh script. stop_server.sh : This script stops and removes all Docker containers closing the pipelines.","title":"Appendix: Benchmark Helper Scripts"},{"location":"OVMS/pipelinerun.html","text":"Customized Run Pipeline Overview When the pipeline is run, the run.sh script starts the service and performs inferencing on the selected input media. The output of running the pipeline provides the inference results for each frame based on the media source such as text, barcode, and so on, as well as the frames per second (FPS). Pipeline run provides many options in media type, system process platform type, and additional optional parameters. These options give you the opportunity to compare what system process platform is better for your need. Start Pipeline You can run the pipeline script, run.sh with a given pipeline profile via the environment variable PIPELINE_PROFILE , and the following additional input parameters: Media type Camera Simulator running using RTSP USB Camera using a supported output format Real Sense Camera using the serial number Video File Platform core dgpu.0 dgpu.1 xeon Environment Variables Run the command based on your requirement. Once choices are selected for #1-3 above, to start the pipeline run, use the commands from the Examples section below. Examples using different input source types In the following examples, environment variables are used to select the desired PIPELINE_PROFILE and RENDER_MODE . This table uses run.sh to run the object_detection pipeline profile: Input source Type Input Source Parameter Command Simulated camera rtsp://127.0.0.1:8554/camera_X PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc rtsp://127.0.0.1:8554/camera_1 RealSense camera <serial_number> --realsense_enabled PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc --realsense_enabled USB camera /dev/video0 PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc /dev/video0 Video file file:my_video_file.mp4 PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc file:my_video_file.mp4 Note The value of x in dgpu.x can be 0, 1, 2, and so on depending on the number of discrete GPUs in the system.","title":"Custom Run Pipelines"},{"location":"OVMS/pipelinerun.html#customized-run-pipeline","text":"","title":"Customized Run Pipeline"},{"location":"OVMS/pipelinerun.html#overview","text":"When the pipeline is run, the run.sh script starts the service and performs inferencing on the selected input media. The output of running the pipeline provides the inference results for each frame based on the media source such as text, barcode, and so on, as well as the frames per second (FPS). Pipeline run provides many options in media type, system process platform type, and additional optional parameters. These options give you the opportunity to compare what system process platform is better for your need.","title":"Overview"},{"location":"OVMS/pipelinerun.html#start-pipeline","text":"You can run the pipeline script, run.sh with a given pipeline profile via the environment variable PIPELINE_PROFILE , and the following additional input parameters: Media type Camera Simulator running using RTSP USB Camera using a supported output format Real Sense Camera using the serial number Video File Platform core dgpu.0 dgpu.1 xeon Environment Variables Run the command based on your requirement. Once choices are selected for #1-3 above, to start the pipeline run, use the commands from the Examples section below.","title":"Start Pipeline"},{"location":"OVMS/pipelinerun.html#examples-using-different-input-source-types","text":"In the following examples, environment variables are used to select the desired PIPELINE_PROFILE and RENDER_MODE . This table uses run.sh to run the object_detection pipeline profile: Input source Type Input Source Parameter Command Simulated camera rtsp://127.0.0.1:8554/camera_X PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc rtsp://127.0.0.1:8554/camera_1 RealSense camera <serial_number> --realsense_enabled PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc --realsense_enabled USB camera /dev/video0 PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc /dev/video0 Video file file:my_video_file.mp4 PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc file:my_video_file.mp4 Note The value of x in dgpu.x can be 0, 1, 2, and so on depending on the number of discrete GPUs in the system.","title":"Examples using different input source types"},{"location":"OVMS/pipelinesetup.html","text":"Set up Pipeline Clone the repository bash git clone https://github.com/intel-retail/automated-self-checkout.git && cd ./automated-self-checkout Build the profile launcher binary executable bash make build-profile-launcher Each profile is an unique pipeline use case. We provide some profile examples, and the configuration examples of profiles are located here . Go here to find out the detail descriptions for the configuration of profile used by profile launcher. Build the benchmark Docker images ```bash cd benchmark-scripts make build-all cd .. ``` Note A successfully built benchmark Docker images should contain the following Docker images from docker images benchmark --format 'table{{.Repository}}\\t{{.Tag}}' command: benchmark:dev benchmark:xpu benchmark:igt Note After successfully built benchmark Docker images, please remember to change the directory back to the project base directory from the current benchmark-scripts directory (i.e. cd .. ) for the following steps. Download the models manually (Optional) Note The model downloader script is automatically called as part of run.sh. bash ./download_models/getModels.sh Warning Depending on your internet connection, this might take some time. (Optional) Download the video file manually. This video is used as the input source to give to the pipeline. Note The sample image downloader script is automatically called as part of run.sh. bash ./configs/opencv-ovms/scripts/image_download.sh Warning Depending on your internet connection, this might take some time. (optional) Download the bit model manually a. Here is the command to build the container for bit model downloading: bash docker build -f Dockerfile.bitModel -t bit_model_downloader:dev . b. Here is the script to run the container that downloads the bit models: bash docker run bit_model_downloader:dev Build the reference design images. This table shows the commands for the OpenVINO (OVMS) model Server and profile-launcher build command: Target Docker Build Command Check Success OVMS Server make build-ovms-server docker images command output contains Docker image openvino/model_server:2023.1-gpu OVMS Profile Launcher make build-profile-launcher ls -al ./profile-launcher command to show the binary executable Note Build command may take a while, depending on your internet connection and machine specifications. Note If the build command succeeds, you will see all the built Docker images files as indicated in the Check Success column. If the build fails, check the console output for errors. Proxy If docker build system requires a proxy network, just set your proxy env standard way on your terminal as below and make build: bash export HTTP_PROXY=\"http://your-proxy-url.com:port\" export HTTPS_PROXY=\"https://your-proxy-url.com:port\" make build-ovms-server make build-profile-launcher","title":"Setup Pipelines"},{"location":"OVMS/pipelinesetup.html#set-up-pipeline","text":"Clone the repository bash git clone https://github.com/intel-retail/automated-self-checkout.git && cd ./automated-self-checkout Build the profile launcher binary executable bash make build-profile-launcher Each profile is an unique pipeline use case. We provide some profile examples, and the configuration examples of profiles are located here . Go here to find out the detail descriptions for the configuration of profile used by profile launcher. Build the benchmark Docker images ```bash cd benchmark-scripts make build-all cd .. ``` Note A successfully built benchmark Docker images should contain the following Docker images from docker images benchmark --format 'table{{.Repository}}\\t{{.Tag}}' command: benchmark:dev benchmark:xpu benchmark:igt Note After successfully built benchmark Docker images, please remember to change the directory back to the project base directory from the current benchmark-scripts directory (i.e. cd .. ) for the following steps. Download the models manually (Optional) Note The model downloader script is automatically called as part of run.sh. bash ./download_models/getModels.sh Warning Depending on your internet connection, this might take some time. (Optional) Download the video file manually. This video is used as the input source to give to the pipeline. Note The sample image downloader script is automatically called as part of run.sh. bash ./configs/opencv-ovms/scripts/image_download.sh Warning Depending on your internet connection, this might take some time. (optional) Download the bit model manually a. Here is the command to build the container for bit model downloading: bash docker build -f Dockerfile.bitModel -t bit_model_downloader:dev . b. Here is the script to run the container that downloads the bit models: bash docker run bit_model_downloader:dev Build the reference design images. This table shows the commands for the OpenVINO (OVMS) model Server and profile-launcher build command: Target Docker Build Command Check Success OVMS Server make build-ovms-server docker images command output contains Docker image openvino/model_server:2023.1-gpu OVMS Profile Launcher make build-profile-launcher ls -al ./profile-launcher command to show the binary executable Note Build command may take a while, depending on your internet connection and machine specifications. Note If the build command succeeds, you will see all the built Docker images files as indicated in the Check Success column. If the build fails, check the console output for errors. Proxy If docker build system requires a proxy network, just set your proxy env standard way on your terminal as below and make build: bash export HTTP_PROXY=\"http://your-proxy-url.com:port\" export HTTPS_PROXY=\"https://your-proxy-url.com:port\" make build-ovms-server make build-profile-launcher","title":"Set up Pipeline"},{"location":"OVMS/profileLauncherConfigs.html","text":"Profile Configuration For the profile launcher, each profile has its own configuration for different pipelines. The configuration of each profile is done through a yaml configuration file, configuration.yaml. One example of configuration.yaml is shown here for classification profile : yaml OvmsSingleContainer: false OvmsServer: ServerDockerScript: start_ovms_server.sh ServerDockerImage: openvino/model_server:2023.1-gpu ServerContainerName: ovms-server ServerConfig: \"/models/config.json\" StartupMessage: Starting OVMS server InitWaitTime: 10s EnvironmentVariableFiles: - ovms_server.env # StartUpPolicy: # when there is an error on launching ovms server startup, choose one of these values for the behavior of profile-launcher: # remove-and-restart: it will remove the existing container with the same container name if any and then restart the container # exit: it will exit the profile-launcher and # ignore: it will ignore the error and continue (this is the default value if not given or none of the above) StartUpPolicy: ignore OvmsClient: DockerLauncher: Script: docker-launcher.sh DockerImage: python-demo:dev ContainerName: classification Volumes: - \"$RUN_PATH/results:/tmp/results\" - ~/.Xauthority:/home/dlstreamer/.Xauthority - /tmp/.X11-unix PipelineScript: ./classification/python/entrypoint.sh PipelineInputArgs: \"\" # space delimited like we run the script in command and take those input arguments EnvironmentVariableFiles: - classification.env The description of each configuration element is explained below: Configuration Element Description OvmsSingleContainer This boolean flag indicates whether this profile is running as a single OpenVino Model Server (OVMS) container or not, e.g. the C-API pipeline use case will use this as true . It can indicate the distributed architecture of OVMS client-server when this flag is false. OvmsServer This is configuration section for OpenVino Model Server in the case of client-server architecture. OvmsServer/ServerDockerScript The infra-structure shell script to start an instance of OVMS server. OvmsServer/ServerDockerImage The Docker image tag name for OpenVino Model Server. OvmsServer/ServerContainerName The Docker container base name for OpenVino Model Server. OvmsServer/ServerConfig The model config.json file name path for OpenVino Model Server. OvmsServer/StartupMessage The starting message shown in the console or log when OpenVino Model Server instance is launched. OvmsServer/InitWaitTime The waiting time duration (like 5s, 5m, .. etc) after OpenVino Model Server is launched to allow some settling time before launching the pipeline from the client. OvmsServer/EnvironmentVariableFiles The list of environment variable files applied for starting OpenVino Model Server Docker instance. OvmsServer/StartUpPolicy This configuration controls the behavior of OpenVino Model Server Docker instance when there is error occurred during launching. Use one of these values: remove-and-restart : it will remove the existing container with the same container name if any and then restart the container exit : it will exit the profile-launcher ignore : it will ignore the error and continue (this is the default value if not given or none of the above). OvmsClient This is configuration section for the OVMS client running pipelines in the case of client-server architecture. The C-API pipeline use case should also use this section to configure. OvmsClient/DockerLauncher This is configuration section for the generic Docker launcher to run pipelines for a given profile. OvmsClient/DockerLauncher/Script The generic Docker launcher script file name. OvmsClient/DockerLauncher/DockerImage The Docker image tag name for the pipeline profile. OvmsClient/DockerLauncher/ContainerName The Docker container base name for the running pipeline profile. OvmsClient/DockerLauncher/Volumes The Docker container volume mounts for the running pipeline profile. OvmsClient/PipelineScript The file name path for the pipeline profile to launch. The file path here is in the perspective of the running container. i.e. the path inside the running container. OvmsClient/PipelineInputArgs Any input arguments or parameters for the above pipeline script to take. Like any command line argument, they are space-delimited if multiple arguments. OvmsClient/EnvironmentVariableFiles The list of environment variable files applied for the running pipeline profile Docker instance.","title":"Profile Configuration"},{"location":"OVMS/profileLauncherConfigs.html#profile-configuration","text":"For the profile launcher, each profile has its own configuration for different pipelines. The configuration of each profile is done through a yaml configuration file, configuration.yaml. One example of configuration.yaml is shown here for classification profile : yaml OvmsSingleContainer: false OvmsServer: ServerDockerScript: start_ovms_server.sh ServerDockerImage: openvino/model_server:2023.1-gpu ServerContainerName: ovms-server ServerConfig: \"/models/config.json\" StartupMessage: Starting OVMS server InitWaitTime: 10s EnvironmentVariableFiles: - ovms_server.env # StartUpPolicy: # when there is an error on launching ovms server startup, choose one of these values for the behavior of profile-launcher: # remove-and-restart: it will remove the existing container with the same container name if any and then restart the container # exit: it will exit the profile-launcher and # ignore: it will ignore the error and continue (this is the default value if not given or none of the above) StartUpPolicy: ignore OvmsClient: DockerLauncher: Script: docker-launcher.sh DockerImage: python-demo:dev ContainerName: classification Volumes: - \"$RUN_PATH/results:/tmp/results\" - ~/.Xauthority:/home/dlstreamer/.Xauthority - /tmp/.X11-unix PipelineScript: ./classification/python/entrypoint.sh PipelineInputArgs: \"\" # space delimited like we run the script in command and take those input arguments EnvironmentVariableFiles: - classification.env The description of each configuration element is explained below: Configuration Element Description OvmsSingleContainer This boolean flag indicates whether this profile is running as a single OpenVino Model Server (OVMS) container or not, e.g. the C-API pipeline use case will use this as true . It can indicate the distributed architecture of OVMS client-server when this flag is false. OvmsServer This is configuration section for OpenVino Model Server in the case of client-server architecture. OvmsServer/ServerDockerScript The infra-structure shell script to start an instance of OVMS server. OvmsServer/ServerDockerImage The Docker image tag name for OpenVino Model Server. OvmsServer/ServerContainerName The Docker container base name for OpenVino Model Server. OvmsServer/ServerConfig The model config.json file name path for OpenVino Model Server. OvmsServer/StartupMessage The starting message shown in the console or log when OpenVino Model Server instance is launched. OvmsServer/InitWaitTime The waiting time duration (like 5s, 5m, .. etc) after OpenVino Model Server is launched to allow some settling time before launching the pipeline from the client. OvmsServer/EnvironmentVariableFiles The list of environment variable files applied for starting OpenVino Model Server Docker instance. OvmsServer/StartUpPolicy This configuration controls the behavior of OpenVino Model Server Docker instance when there is error occurred during launching. Use one of these values: remove-and-restart : it will remove the existing container with the same container name if any and then restart the container exit : it will exit the profile-launcher ignore : it will ignore the error and continue (this is the default value if not given or none of the above). OvmsClient This is configuration section for the OVMS client running pipelines in the case of client-server architecture. The C-API pipeline use case should also use this section to configure. OvmsClient/DockerLauncher This is configuration section for the generic Docker launcher to run pipelines for a given profile. OvmsClient/DockerLauncher/Script The generic Docker launcher script file name. OvmsClient/DockerLauncher/DockerImage The Docker image tag name for the pipeline profile. OvmsClient/DockerLauncher/ContainerName The Docker container base name for the running pipeline profile. OvmsClient/DockerLauncher/Volumes The Docker container volume mounts for the running pipeline profile. OvmsClient/PipelineScript The file name path for the pipeline profile to launch. The file path here is in the perspective of the running container. i.e. the path inside the running container. OvmsClient/PipelineInputArgs Any input arguments or parameters for the above pipeline script to take. Like any command line argument, they are space-delimited if multiple arguments. OvmsClient/EnvironmentVariableFiles The list of environment variable files applied for the running pipeline profile Docker instance.","title":"Profile Configuration"},{"location":"OVMS/quick_pipelinerun.html","text":"Quick Start Guide to Run Pipeline Prerequisites Before running, set up the pipeline . Running OVMS with the camera simulator Start the Camera Simulator Download the video files to the sample-media directory: bash cd benchmark-scripts; ./download_sample_videos.sh; cd ..; Example - Specify Resolution and Framerate This example downloads a sample video for 1080p@15fps. bash cd benchmark-scripts; ./download_sample_videos.sh 1920 1080 15; cd ..; Note Only AVC encoded files are supported. After the video files are downloaded to the sample-media folder, start the camera simulator: bash make run-camera-simulator Wait for few seconds, and then check if the camera-simulator containers are running: bash docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}' Success Your output is as follows: IMAGE STATUS NAMES openvino/ubuntu20_data_runtime:2021.4.2 Up 11 seconds camera-simulator0 aler9/rtsp-simple-server Up 13 seconds camera-simulator Note There could be multiple containers with the image \"openvino/ubuntu20_data_runtime:2021.4.2\", depending on the number of sample-media video files provided. Failure If all the Docker* containers are not visible, then review the console output for errors. Sometimes dependencies fail to resolve. Address obvious issues and retry. Run Instance Segmentation There are several pipeline profiles to chose from. Use the make list-profiles to see the different pipeline options. In this example, the instance_segmentation pipeline profile will be used. Use the following command to run instance segmentation using OVMS on core. bash PIPELINE_PROFILE=\"instance_segmentation\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 Check the status of the pipeline. bash docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}' -a Success Here is a sample output: IMAGE STATUS NAMES openvino/model_server-gpu:latest Up 59 seconds ovms-server0 Failure If you do not see above Docker container(s), review the console output for errors. Sometimes dependencies fail to resolve and must be run again. Address obvious issues and try again repeating the above steps. Here are couple debugging tips: check the docker logs using following command to see if there is an issue with the container bash docker logs <containerName> check ovms log in automated-self-checkout/results/r0.jsonl Check the output in the results directory. Example - results/r0.jsonl sample The output in results/r0.jsonl file lists average processing time in milliseconds and average number of frames per second. This file is intended for scripts to parse. text Processing time: 53.17 ms; fps: 18.81 Processing time: 47.98 ms; fps: 20.84 Processing time: 48.35 ms; fps: 20.68 Processing time: 46.88 ms; fps: 21.33 Processing time: 47.56 ms; fps: 21.03 Processing time: 49.66 ms; fps: 20.14 Processing time: 52.49 ms; fps: 19.05 Processing time: 52.27 ms; fps: 19.13 Processing time: 50.86 ms; fps: 19.66 Processing time: 58.19 ms; fps: 17.18 Processing time: 58.28 ms; fps: 17.16 Processing time: 52.17 ms; fps: 19.17 Processing time: 50.89 ms; fps: 19.65 Processing time: 49.58 ms; fps: 20.17 Processing time: 51.14 ms; fps: 19.55 Example - results/pipeline0.log sample The output in results/pipeline0.log lists average number of frames per second. Below is a snap shot of the output: text 18.81 20.84 20.68 21.33 21.03 20.14 19.05 19.13 19.66 17.18 17.16 19.17 19.65 20.17 19.55 Note The automated-self-checkout/results/ directory is volume mounted to the pipeline container. Stop running the pipelines To stop the instance segmentation container and clean up, run bash make clean-all","title":"Quick Start Run Pipeline"},{"location":"OVMS/quick_pipelinerun.html#quick-start-guide-to-run-pipeline","text":"","title":"Quick Start Guide to Run Pipeline"},{"location":"OVMS/quick_pipelinerun.html#prerequisites","text":"Before running, set up the pipeline .","title":"Prerequisites"},{"location":"OVMS/quick_pipelinerun.html#running-ovms-with-the-camera-simulator","text":"","title":"Running OVMS with the camera simulator"},{"location":"OVMS/quick_pipelinerun.html#start-the-camera-simulator","text":"Download the video files to the sample-media directory: bash cd benchmark-scripts; ./download_sample_videos.sh; cd ..; Example - Specify Resolution and Framerate This example downloads a sample video for 1080p@15fps. bash cd benchmark-scripts; ./download_sample_videos.sh 1920 1080 15; cd ..; Note Only AVC encoded files are supported. After the video files are downloaded to the sample-media folder, start the camera simulator: bash make run-camera-simulator Wait for few seconds, and then check if the camera-simulator containers are running: bash docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}' Success Your output is as follows: IMAGE STATUS NAMES openvino/ubuntu20_data_runtime:2021.4.2 Up 11 seconds camera-simulator0 aler9/rtsp-simple-server Up 13 seconds camera-simulator Note There could be multiple containers with the image \"openvino/ubuntu20_data_runtime:2021.4.2\", depending on the number of sample-media video files provided. Failure If all the Docker* containers are not visible, then review the console output for errors. Sometimes dependencies fail to resolve. Address obvious issues and retry.","title":"Start the Camera Simulator"},{"location":"OVMS/quick_pipelinerun.html#run-instance-segmentation","text":"There are several pipeline profiles to chose from. Use the make list-profiles to see the different pipeline options. In this example, the instance_segmentation pipeline profile will be used. Use the following command to run instance segmentation using OVMS on core. bash PIPELINE_PROFILE=\"instance_segmentation\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 Check the status of the pipeline. bash docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}' -a Success Here is a sample output: IMAGE STATUS NAMES openvino/model_server-gpu:latest Up 59 seconds ovms-server0 Failure If you do not see above Docker container(s), review the console output for errors. Sometimes dependencies fail to resolve and must be run again. Address obvious issues and try again repeating the above steps. Here are couple debugging tips: check the docker logs using following command to see if there is an issue with the container bash docker logs <containerName> check ovms log in automated-self-checkout/results/r0.jsonl Check the output in the results directory. Example - results/r0.jsonl sample The output in results/r0.jsonl file lists average processing time in milliseconds and average number of frames per second. This file is intended for scripts to parse. text Processing time: 53.17 ms; fps: 18.81 Processing time: 47.98 ms; fps: 20.84 Processing time: 48.35 ms; fps: 20.68 Processing time: 46.88 ms; fps: 21.33 Processing time: 47.56 ms; fps: 21.03 Processing time: 49.66 ms; fps: 20.14 Processing time: 52.49 ms; fps: 19.05 Processing time: 52.27 ms; fps: 19.13 Processing time: 50.86 ms; fps: 19.66 Processing time: 58.19 ms; fps: 17.18 Processing time: 58.28 ms; fps: 17.16 Processing time: 52.17 ms; fps: 19.17 Processing time: 50.89 ms; fps: 19.65 Processing time: 49.58 ms; fps: 20.17 Processing time: 51.14 ms; fps: 19.55 Example - results/pipeline0.log sample The output in results/pipeline0.log lists average number of frames per second. Below is a snap shot of the output: text 18.81 20.84 20.68 21.33 21.03 20.14 19.05 19.13 19.66 17.18 17.16 19.17 19.65 20.17 19.55 Note The automated-self-checkout/results/ directory is volume mounted to the pipeline container.","title":"Run Instance Segmentation"},{"location":"OVMS/quick_pipelinerun.html#stop-running-the-pipelines","text":"To stop the instance segmentation container and clean up, run bash make clean-all","title":"Stop running the pipelines"},{"location":"OVMS/quick_stream_density.html","text":"Quick Start Guide to Run Pipeline Stream Density In this section, we show the steps to run the stream density for a chosen pipeline profile. By definition, the objective of the stream density is to bench-mark the maximum number of multiple running pipelines at the same time while still maintaining the goal-setting target frames-per-second (FPS). Prerequisites Before running, set up the pipeline if not already done. Stop All Other Running Pipelines To make sure we have a good stream density benchmarking, it is recommended to stop all other running pipelines before running the stream density. To stop all running pipelines and clean up, run bash make clean-all Build Benchmark Docker Images For running stream density, the benchmark scripts are utilized. To set up the benchmarking, we need to build the benchmark Docker images first. Build the benchmark Docker* images Benchmark scripts are containerized inside Docker. The easiest way to build all benchmark Docker images, run bash cd ./benchmark-scripts make It is also possible to choose which benchmark Docker images to build based on different platforms. The following table lists the commands for various platforms. Choose and run the command corresponding to your hardware configuration. Platform Docker Build Command Check Success Intel\u00ae integrated and Arc\u2122 GPUs cd benchmark-scripts make build-benchmark make build-igt Docker images command to show both benchmark:dev and benchmark:igt images Intel\u00ae Flex GPUs cd benchmark-scripts make build-benchmark make build-xpu Docker images command to show both benchmark:dev and benchmark:xpu images Warning Build command may take a while, depending on your internet connection and machine specifications. Start the Camera Simulator We will use the camera simulator as the input source to show the stream density. Please refer to the section of Start the Camera Simulator in Quick Start Guide to Run Pipeline on how to start the camera simulator. Run Objection Detection Pipeline Stream Density There are several pipeline profiles to choose from for running pipeline stream density. Use the make list-profiles to see the different pipeline options. In this example, the object_detection pipeline profile will be used for running stream density. To run the stream density, the benchmark shell script, benchmark.sh , is used. The script is in the <project_base_dir> / benchmark-scripts directory. Use the following command to run objection detection pipeline profile using OVMS on core. bash cd ./benchmark-scripts PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --stream_density 15.0 --logdir object_detection/data --duration 120 --init_duration 40 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1 Note Description of some key benchmarking input parameters is given as below: Parameter Name Example Value Description --stream_density 15.0 The value 15.0 after the --stream_density is the target FPS that we want to achieve for running maximum number of object detection pipelines while the averaged of all pipelines from the output FPS still maintaining that target FPS value. --logdir object_detection/data the output directory of benchmarking resource details --duration 120 the time duration, in number of seconds, the benchmarking will run --init_duration 40 the time duration, in number of seconds, to wait for system initialization before the benchmarking metrics or data collection begins Note For stream density run, it is recommended to turn off the display to conserve the system resources hence setting RENDER_MODE=0 Note This takes a while for the whole stream density benchmarking process depending on your system resources like CPU, memory, ... etc. Note The benchmark.sh script automatically cleans all running Docker containers after it is done. If the hardware supports, then one can also run the benchmarking on different devices like CPU or GPU. This can be done through the environment variable DEVICE . The following is an example to run the object_detection profile using GPU: bash PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 DEVICE=\"GPU.0\" sudo -E ./benchmark.sh --stream_density 14.95 --logdir object_detection/data --duration 120 --init_duration 40 --platform dgpu.0 --inputsrc rtsp://127.0.0.1:8554/camera_1 Note The performance of running object detection benchmarking should be better while running on GPU using model precision FP32. If supported, then you can change the model precision by going to the folder configs/opencv-ovms/models/2022 from the root of project folder and editing the base_path for that particular model in the config_template.json file. For example, you can change the the base_path of FP32 to FP16 assuming the precision FP16 of the model is available: ```json ... \"config\": { \"name\": \"ssd_mobilenet_v1_coco\", \"base_path\": \"/models/ssd_mobilenet_v1_coco/FP32\", ... } ``` The directory structure of models with both precisions should look like the followings: ```console ssd_mobilenet_v1_coco \u251c\u2500\u2500 FP32 | \u2514\u2500\u2500 1 | \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin | \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml \u251c\u2500\u2500 FP16 | \u2514\u2500\u2500 1 | \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin | \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml ``` Check the output in the base results directory. After the stream density is done, the results of stream density can be seen on the base directory of the results directory: bash cat ../results/stream_density.log Example - results/stream_density.log sample The output in results/stream_density.log file gives the detailed information of stream density results: text ...... FPS for pipeline0: 15.1225 FPS for pipeline1: 15.19 FPS for pipeline2: 15.18 Total FPS throughput: 45.4925 Total FPS per stream: 15.1642 Max stream density achieved for target FPS 15.0 is 3 Finished stream density benchmarking stream_density done!","title":"Stream Density Quick Start Guide"},{"location":"OVMS/quick_stream_density.html#quick-start-guide-to-run-pipeline-stream-density","text":"In this section, we show the steps to run the stream density for a chosen pipeline profile. By definition, the objective of the stream density is to bench-mark the maximum number of multiple running pipelines at the same time while still maintaining the goal-setting target frames-per-second (FPS).","title":"Quick Start Guide to Run Pipeline Stream Density"},{"location":"OVMS/quick_stream_density.html#prerequisites","text":"Before running, set up the pipeline if not already done.","title":"Prerequisites"},{"location":"OVMS/quick_stream_density.html#stop-all-other-running-pipelines","text":"To make sure we have a good stream density benchmarking, it is recommended to stop all other running pipelines before running the stream density. To stop all running pipelines and clean up, run bash make clean-all","title":"Stop All Other Running Pipelines"},{"location":"OVMS/quick_stream_density.html#build-benchmark-docker-images","text":"For running stream density, the benchmark scripts are utilized. To set up the benchmarking, we need to build the benchmark Docker images first. Build the benchmark Docker* images Benchmark scripts are containerized inside Docker. The easiest way to build all benchmark Docker images, run bash cd ./benchmark-scripts make It is also possible to choose which benchmark Docker images to build based on different platforms. The following table lists the commands for various platforms. Choose and run the command corresponding to your hardware configuration. Platform Docker Build Command Check Success Intel\u00ae integrated and Arc\u2122 GPUs cd benchmark-scripts make build-benchmark make build-igt Docker images command to show both benchmark:dev and benchmark:igt images Intel\u00ae Flex GPUs cd benchmark-scripts make build-benchmark make build-xpu Docker images command to show both benchmark:dev and benchmark:xpu images Warning Build command may take a while, depending on your internet connection and machine specifications.","title":"Build Benchmark Docker Images"},{"location":"OVMS/quick_stream_density.html#start-the-camera-simulator","text":"We will use the camera simulator as the input source to show the stream density. Please refer to the section of Start the Camera Simulator in Quick Start Guide to Run Pipeline on how to start the camera simulator.","title":"Start the Camera Simulator"},{"location":"OVMS/quick_stream_density.html#run-objection-detection-pipeline-stream-density","text":"There are several pipeline profiles to choose from for running pipeline stream density. Use the make list-profiles to see the different pipeline options. In this example, the object_detection pipeline profile will be used for running stream density. To run the stream density, the benchmark shell script, benchmark.sh , is used. The script is in the <project_base_dir> / benchmark-scripts directory. Use the following command to run objection detection pipeline profile using OVMS on core. bash cd ./benchmark-scripts PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --stream_density 15.0 --logdir object_detection/data --duration 120 --init_duration 40 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1 Note Description of some key benchmarking input parameters is given as below: Parameter Name Example Value Description --stream_density 15.0 The value 15.0 after the --stream_density is the target FPS that we want to achieve for running maximum number of object detection pipelines while the averaged of all pipelines from the output FPS still maintaining that target FPS value. --logdir object_detection/data the output directory of benchmarking resource details --duration 120 the time duration, in number of seconds, the benchmarking will run --init_duration 40 the time duration, in number of seconds, to wait for system initialization before the benchmarking metrics or data collection begins Note For stream density run, it is recommended to turn off the display to conserve the system resources hence setting RENDER_MODE=0 Note This takes a while for the whole stream density benchmarking process depending on your system resources like CPU, memory, ... etc. Note The benchmark.sh script automatically cleans all running Docker containers after it is done. If the hardware supports, then one can also run the benchmarking on different devices like CPU or GPU. This can be done through the environment variable DEVICE . The following is an example to run the object_detection profile using GPU: bash PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 DEVICE=\"GPU.0\" sudo -E ./benchmark.sh --stream_density 14.95 --logdir object_detection/data --duration 120 --init_duration 40 --platform dgpu.0 --inputsrc rtsp://127.0.0.1:8554/camera_1 Note The performance of running object detection benchmarking should be better while running on GPU using model precision FP32. If supported, then you can change the model precision by going to the folder configs/opencv-ovms/models/2022 from the root of project folder and editing the base_path for that particular model in the config_template.json file. For example, you can change the the base_path of FP32 to FP16 assuming the precision FP16 of the model is available: ```json ... \"config\": { \"name\": \"ssd_mobilenet_v1_coco\", \"base_path\": \"/models/ssd_mobilenet_v1_coco/FP32\", ... } ``` The directory structure of models with both precisions should look like the followings: ```console ssd_mobilenet_v1_coco \u251c\u2500\u2500 FP32 | \u2514\u2500\u2500 1 | \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin | \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml \u251c\u2500\u2500 FP16 | \u2514\u2500\u2500 1 | \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin | \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml ``` Check the output in the base results directory. After the stream density is done, the results of stream density can be seen on the base directory of the results directory: bash cat ../results/stream_density.log Example - results/stream_density.log sample The output in results/stream_density.log file gives the detailed information of stream density results: text ...... FPS for pipeline0: 15.1225 FPS for pipeline1: 15.19 FPS for pipeline2: 15.18 Total FPS throughput: 45.4925 Total FPS per stream: 15.1642 Max stream density achieved for target FPS 15.0 is 3 Finished stream density benchmarking stream_density done!","title":"Run Objection Detection Pipeline Stream Density"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html","text":"Run Object Detection Pipeline with New Model OpenVINO Model Server has many ways to run inferencing pipeline : TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). For running object detection pipeline, it is based on KServe gRPC API method, default model used is ssd_mobilenet_v1_coco. You can use different model to run object detection. Here are the steps: Add new section to config file for model server Download new model Update environment variables of detection pipeline for new model Build and Run Add New Section to Config File for Model Server Here is the config file location: configs/opencv-ovms/models/2022/config_template.json , edit the file and append the following configuration section template json , { \"config\": { \"name\": \"ssd_mobilenet_v1_coco\", \"base_path\": \"/models/ssd_mobilenet_v1_coco/FP32\", \"nireq\": 1, \"batch_size\": \"1\", \"plugin_config\": { \"PERFORMANCE_HINT\": \"LATENCY\" }, \"target_device\": \"{target_device}\" }, \"latest\": { \"num_versions\": 1 } } Note Please leave target_device value as it is, as the value {target_device} will be recognized and replaced by script run. You can find the parameter description in the ovms docs . Download New Model The pipeline run script automatically download the model files if it is part of open model zoo supported list ; otherwise, please add your model files manually to configs/opencv-ovms/models/2022/ . When you add your model manually, make sure to follow the model file structure as / /1/modelfiles, for example: text ssd_mobilenet_v1_coco \u251c\u2500\u2500 FP32 \u2514\u2500\u2500 1 \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml Update Environment Variables You can update the object detection environment variables in file: configs/opencv-ovms/envs/object_detection.env , here is default value and explanation for each environment variable: EV Name Default Value Description DETECTION_MODEL_NAME ssd_mobilenet_v1_coco model name for object detection DETECTION_LABEL_FILE coco_91cl_bkgr.txt label file name to use on object detection for model DETECTION_ARCHITECTURE_TYPE ssd architecture type for object detection model DETECTION_OUTPUT_RESOLUTION 1920x1080 output resolution for object detection result DETECTION_THRESHOLD 0.50 threshold for object detection in floating point that needs to be between 0.0 to 1.0 MQTT enable MQTT notification of result, value: empty RENDER_MODE 1 display the input source video stream with the inferencing results, value: 0 Build and Run Pipeline Build the python app and profile-launcher: make build-python-apps Download sample video files: cd benchmark-scripts/ && ./download_sample_videos.sh && cd .. Start simulator camera if not started: make run-camera-simulator (Optional) Run MQTT broker: docker run --network host --rm -d -it -p 1883:1883 -p 9001:9001 eclipse-mosquitto To start object detection pipeline: PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 MQTT=127.0.0.1:1883 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 (remove the MQTT environment variable if not using it) If do use MQTT, use the container name as the MQTT topic to subscribe to the inference metadata. Do a docker ps to know the container name. To stop the running pipelines: make clean-profile-launcher to stop and clean up the client side containers, or make clean-all to stop and clean up everything.","title":"Object Detection Pipeline"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#run-object-detection-pipeline-with-new-model","text":"OpenVINO Model Server has many ways to run inferencing pipeline : TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). For running object detection pipeline, it is based on KServe gRPC API method, default model used is ssd_mobilenet_v1_coco. You can use different model to run object detection. Here are the steps: Add new section to config file for model server Download new model Update environment variables of detection pipeline for new model Build and Run","title":"Run Object Detection Pipeline with New Model"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#add-new-section-to-config-file-for-model-server","text":"Here is the config file location: configs/opencv-ovms/models/2022/config_template.json , edit the file and append the following configuration section template json , { \"config\": { \"name\": \"ssd_mobilenet_v1_coco\", \"base_path\": \"/models/ssd_mobilenet_v1_coco/FP32\", \"nireq\": 1, \"batch_size\": \"1\", \"plugin_config\": { \"PERFORMANCE_HINT\": \"LATENCY\" }, \"target_device\": \"{target_device}\" }, \"latest\": { \"num_versions\": 1 } } Note Please leave target_device value as it is, as the value {target_device} will be recognized and replaced by script run. You can find the parameter description in the ovms docs .","title":"Add New Section to Config File for Model Server"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#download-new-model","text":"The pipeline run script automatically download the model files if it is part of open model zoo supported list ; otherwise, please add your model files manually to configs/opencv-ovms/models/2022/ . When you add your model manually, make sure to follow the model file structure as / /1/modelfiles, for example: text ssd_mobilenet_v1_coco \u251c\u2500\u2500 FP32 \u2514\u2500\u2500 1 \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml","title":"Download New Model"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#update-environment-variables","text":"You can update the object detection environment variables in file: configs/opencv-ovms/envs/object_detection.env , here is default value and explanation for each environment variable: EV Name Default Value Description DETECTION_MODEL_NAME ssd_mobilenet_v1_coco model name for object detection DETECTION_LABEL_FILE coco_91cl_bkgr.txt label file name to use on object detection for model DETECTION_ARCHITECTURE_TYPE ssd architecture type for object detection model DETECTION_OUTPUT_RESOLUTION 1920x1080 output resolution for object detection result DETECTION_THRESHOLD 0.50 threshold for object detection in floating point that needs to be between 0.0 to 1.0 MQTT enable MQTT notification of result, value: empty RENDER_MODE 1 display the input source video stream with the inferencing results, value: 0","title":"Update Environment Variables"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#build-and-run-pipeline","text":"Build the python app and profile-launcher: make build-python-apps Download sample video files: cd benchmark-scripts/ && ./download_sample_videos.sh && cd .. Start simulator camera if not started: make run-camera-simulator (Optional) Run MQTT broker: docker run --network host --rm -d -it -p 1883:1883 -p 9001:9001 eclipse-mosquitto To start object detection pipeline: PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 MQTT=127.0.0.1:1883 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 (remove the MQTT environment variable if not using it) If do use MQTT, use the container name as the MQTT topic to subscribe to the inference metadata. Do a docker ps to know the container name. To stop the running pipelines: make clean-profile-launcher to stop and clean up the client side containers, or make clean-all to stop and clean up everything.","title":"Build and Run Pipeline"},{"location":"OVMS/stop_pipeline_run.html","text":"Stop pipeline run You can call make clean-ovms to stop the pipeline and all running containers for OVMS, hence the results directory log files will stop growing. Below is the table of make commands you can call to clean things up per your needs: Clean-Up Options Command clean instance-segmentation container if any make clean-segmentation clean grpc-go dev container if any make clean-grpc-go clean all related containers launched by profile-launcher if any make clean-profile-launcher clean ovms-server container make clean-ovms-server clean ovms-server and all containers launched by profile-launcher make clean-ovms clean results/ folder make clean-results clean all downloaded models make clean-models stops pipelines and cleans all containers, simulator, results, telemetry and webcam make clean-all","title":"Stop Pipelines"},{"location":"OVMS/stop_pipeline_run.html#stop-pipeline-run","text":"You can call make clean-ovms to stop the pipeline and all running containers for OVMS, hence the results directory log files will stop growing. Below is the table of make commands you can call to clean things up per your needs: Clean-Up Options Command clean instance-segmentation container if any make clean-segmentation clean grpc-go dev container if any make clean-grpc-go clean all related containers launched by profile-launcher if any make clean-profile-launcher clean ovms-server container make clean-ovms-server clean ovms-server and all containers launched by profile-launcher make clean-ovms clean results/ folder make clean-results clean all downloaded models make clean-models stops pipelines and cleans all containers, simulator, results, telemetry and webcam make clean-all","title":"Stop pipeline run"},{"location":"OVMS/supportingDifferentLanguage.html","text":"Supporting Different Languages For running OVMS as inferencing engine through grpc, these are the supported languages: python golang Sample Commands using the Camera Simulator Input source Type Command Python PIPELINE_PROFILE=\"grpc_python\" sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1 Golang PIPELINE_PROFILE=\"grpc_go\" sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1 Note Above example scripts are based on camera simulator for rtsp input source, before running them, please run the camera simulator . If you used a different input source, fill in the appropriate value for --inputsrc .","title":"Supporting Different Languages"},{"location":"OVMS/supportingDifferentLanguage.html#supporting-different-languages","text":"For running OVMS as inferencing engine through grpc, these are the supported languages: python golang","title":"Supporting Different Languages"},{"location":"OVMS/supportingDifferentLanguage.html#sample-commands-using-the-camera-simulator","text":"Input source Type Command Python PIPELINE_PROFILE=\"grpc_python\" sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1 Golang PIPELINE_PROFILE=\"grpc_go\" sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1 Note Above example scripts are based on camera simulator for rtsp input source, before running them, please run the camera simulator . If you used a different input source, fill in the appropriate value for --inputsrc .","title":"Sample Commands using the Camera Simulator"},{"location":"OVMS/supportingDifferentModel.html","text":"Supporting Different Models For running OVMS as inferencing engine through grpc, we are supporting different models for your need. Models Supported In Python Here is the list of inferencing models we are currently supporting in python: instance-segmentation-security-1040 bit_64 You can switch between them by editing the configuration file configs/opencv-ovms/cmd_client/res/grpc_python/configuration.yaml , uncomment # PipelineInputArgs: \"--model_name instance-segmentation-security-1040\" for supporting instance-segmentation-security-1040 and comment out rest; or you can uncomment # PipelineInputArgs: \"--model_name bit_64\" for supporting bit_64 and comment out rest. Here is the configuration.yaml content, default to use instance-segmentation-security-1040 model ``` OvmsClient: PipelineScript: run_grpc_python.sh PipelineInputArgs: \"--model_name instance-segmentation-security-1040\" # space delimited like we run the script in command and take those input arguments # PipelineInputArgs: \"--model_name bit_64\" # space delimited like we run the script in command and take those input arguments # PipelineInputArgs: \"--model_name yolov5s\" # space delimited like we run the script in command and take those input arguments ``` Download Models You can download models by editing download_models/models.lst file, you can add new models(from https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/object_detection_demo/python/models.lst) to it or uncomment from existing list in this file, saved the file once editing is done. Then you can download the list using following steps: cd download_models make build make run after above steps, the downloaded models can be found in configs/opencv-ovms/models/2022 directory. Note Model files in configs/opencv-ovms/models/2022 directory will be replaced with new downloads if previously existed.","title":"Supporting Different Models"},{"location":"OVMS/supportingDifferentModel.html#supporting-different-models","text":"For running OVMS as inferencing engine through grpc, we are supporting different models for your need.","title":"Supporting Different Models"},{"location":"OVMS/supportingDifferentModel.html#models-supported-in-python","text":"Here is the list of inferencing models we are currently supporting in python: instance-segmentation-security-1040 bit_64 You can switch between them by editing the configuration file configs/opencv-ovms/cmd_client/res/grpc_python/configuration.yaml , uncomment # PipelineInputArgs: \"--model_name instance-segmentation-security-1040\" for supporting instance-segmentation-security-1040 and comment out rest; or you can uncomment # PipelineInputArgs: \"--model_name bit_64\" for supporting bit_64 and comment out rest. Here is the configuration.yaml content, default to use instance-segmentation-security-1040 model ``` OvmsClient: PipelineScript: run_grpc_python.sh PipelineInputArgs: \"--model_name instance-segmentation-security-1040\" # space delimited like we run the script in command and take those input arguments # PipelineInputArgs: \"--model_name bit_64\" # space delimited like we run the script in command and take those input arguments # PipelineInputArgs: \"--model_name yolov5s\" # space delimited like we run the script in command and take those input arguments ```","title":"Models Supported In Python"},{"location":"OVMS/supportingDifferentModel.html#download-models","text":"You can download models by editing download_models/models.lst file, you can add new models(from https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/object_detection_demo/python/models.lst) to it or uncomment from existing list in this file, saved the file once editing is done. Then you can download the list using following steps: cd download_models make build make run after above steps, the downloaded models can be found in configs/opencv-ovms/models/2022 directory. Note Model files in configs/opencv-ovms/models/2022 directory will be replaced with new downloads if previously existed.","title":"Download Models"},{"location":"dev-tools/environment_variables.html","text":"Environment Variables (EVs) The table below lists the environment variables (EVs) that can be used as inputs for the container running the inferencing pipeline. GST profile EVs Common EVs This list of EVs specifically supports the GST profile DLStreamer workloads. Variable Description Values AGGREGATE aggregate branches of the gstreamer pipeline, if any, at the end of the pipeline \"\", \"gvametaaggregate name=aggregate\", \"aggregate branch. ! queue\" BARCODE_RECLASSIFY_INTERVAL time interval in seconds for barcode classification Ex: 5 CLASSIFICATION_OPTIONS extra classification pipeline instruction parameters \"\", \"reclassify-interval=1 batch-size=1 nireq=4 gpu-throughput-streams=4\" CPU_SW_DECODER force to use software decoder for gst-launch decoding video frames in CPU \"force-sw-decoders=1\" DECODE decoding element instructions for gst-launch to use Ex: \"decode bin force-sw-decoders=1\" DETECTION_OPTIONS extra object detection pipeline instruction parameters \"\", \"gpu-throughput-streams=4 nireq=4 batch-size=1\" GST_DEBUG for running pipeline in gst debugging mode 0, 1 GST_PIPELINE_LAUNCH for launching gst pipeline script file path and name Ex: \"/home/pipeline-server/framework-pipelines/yolov5_pipeline/yolov5s.sh\" LOG_LEVEL log level to be set when running gst pipeline ERROR, INFO, WARNING, and more OCR_RECLASSIFY_INTERVAL time interval in seconds for OCR classification Ex: 5 PARALLEL_PIPELINE run pipeline in parallel using the tee branch \"\", \"! tee name=branch ! queue\" PARALLEL_AGGRAGATE aggregate parallel pipeline results together, paired use with PARALLEL_PIPELINE \"\", \"! gvametaaggregate name=aggregate ! gvametaconvert name=metaconvert add-empty-results=true ! gvametapublish name=destination file-format=2 file-path=/tmp/results/r$cid_count.jsonl ! fpsdisplaysink video-sink=fakesink sync=true --verbose branch. ! queue !\" VA_SURFACE use video analytics surface from the shared memory if applicable \"\", \"! \"video/x-raw(memory This list of EVs is common for all profiles. Variable Description Values AUTO_SCALE_FLEX_140 allow workload to manage autoscaling 1, 0 CPU_ONLY for overriding inference to be performed on CPU only 1, 0 DEVICE for setting device to use for pipeline run \"GPU\", \"CPU\", \"AUTO\", \"MULTI LOW_POWER for running pipelines using GPUs only 1, 0 OCR_DEVICE optical character recognition device \"CPU\", \"GPU\" PRE_PROCESS pre process command to add for inferencing \"pre-process-backend=vaapi-surface-sharing\", \"pre-process-backend=vaapi-surface-sharing pre-process-config=VAAPI_FAST_SCALE_LOAD_FACTOR=1\" PIPELINE_PROFILE for choosing OVMS workload's pipeline profile to run use make list-profiles to see Values RENDER_MODE for displaying pipeline and overlay CV metadata 1, 0 STREAM_DENSITY_MODE for starting pipeline stream density testing 1, 0 STREAM_DENSITY_FPS for setting stream density target fps value Ex: 15.0 STREAM_DENSITY_INCREMENTS for setting incrementing number of pipelines for running stream density Ex: 1 Applying EV to Run Pipeline EV can be applied in two ways: 1. as parameter input to run . sh script 2. in the env files The input parameter will override the one in the env files if both are used. EV as input parameter Example bash PIPELINE_PROFILE=\"object_detection\" CPU_ONLY=1 RENDER_MODE=0 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 Note Those EVs in front of run.sh like CPU_ONLY , RENDER_MODE are applied to this run only and they are also known as command line environment overrides, or environment overrides. They will override the default values in env files if any. Editing the Env Files EV can be configured for advanced user in configs/opencv-ovms/envs/ . As an example for gst pipeline profile, there are two Env files can be configured: - ` yolov5 - cpu . env ` file for running pipeline in core system - ` yolov5 - gpu . env ` file for running pipeline in gpu or multi These two files currently hold the default values.","title":"Environment Variables"},{"location":"dev-tools/environment_variables.html#environment-variables-evs","text":"The table below lists the environment variables (EVs) that can be used as inputs for the container running the inferencing pipeline. GST profile EVs Common EVs This list of EVs specifically supports the GST profile DLStreamer workloads. Variable Description Values AGGREGATE aggregate branches of the gstreamer pipeline, if any, at the end of the pipeline \"\", \"gvametaaggregate name=aggregate\", \"aggregate branch. ! queue\" BARCODE_RECLASSIFY_INTERVAL time interval in seconds for barcode classification Ex: 5 CLASSIFICATION_OPTIONS extra classification pipeline instruction parameters \"\", \"reclassify-interval=1 batch-size=1 nireq=4 gpu-throughput-streams=4\" CPU_SW_DECODER force to use software decoder for gst-launch decoding video frames in CPU \"force-sw-decoders=1\" DECODE decoding element instructions for gst-launch to use Ex: \"decode bin force-sw-decoders=1\" DETECTION_OPTIONS extra object detection pipeline instruction parameters \"\", \"gpu-throughput-streams=4 nireq=4 batch-size=1\" GST_DEBUG for running pipeline in gst debugging mode 0, 1 GST_PIPELINE_LAUNCH for launching gst pipeline script file path and name Ex: \"/home/pipeline-server/framework-pipelines/yolov5_pipeline/yolov5s.sh\" LOG_LEVEL log level to be set when running gst pipeline ERROR, INFO, WARNING, and more OCR_RECLASSIFY_INTERVAL time interval in seconds for OCR classification Ex: 5 PARALLEL_PIPELINE run pipeline in parallel using the tee branch \"\", \"! tee name=branch ! queue\" PARALLEL_AGGRAGATE aggregate parallel pipeline results together, paired use with PARALLEL_PIPELINE \"\", \"! gvametaaggregate name=aggregate ! gvametaconvert name=metaconvert add-empty-results=true ! gvametapublish name=destination file-format=2 file-path=/tmp/results/r$cid_count.jsonl ! fpsdisplaysink video-sink=fakesink sync=true --verbose branch. ! queue !\" VA_SURFACE use video analytics surface from the shared memory if applicable \"\", \"! \"video/x-raw(memory This list of EVs is common for all profiles. Variable Description Values AUTO_SCALE_FLEX_140 allow workload to manage autoscaling 1, 0 CPU_ONLY for overriding inference to be performed on CPU only 1, 0 DEVICE for setting device to use for pipeline run \"GPU\", \"CPU\", \"AUTO\", \"MULTI LOW_POWER for running pipelines using GPUs only 1, 0 OCR_DEVICE optical character recognition device \"CPU\", \"GPU\" PRE_PROCESS pre process command to add for inferencing \"pre-process-backend=vaapi-surface-sharing\", \"pre-process-backend=vaapi-surface-sharing pre-process-config=VAAPI_FAST_SCALE_LOAD_FACTOR=1\" PIPELINE_PROFILE for choosing OVMS workload's pipeline profile to run use make list-profiles to see Values RENDER_MODE for displaying pipeline and overlay CV metadata 1, 0 STREAM_DENSITY_MODE for starting pipeline stream density testing 1, 0 STREAM_DENSITY_FPS for setting stream density target fps value Ex: 15.0 STREAM_DENSITY_INCREMENTS for setting incrementing number of pipelines for running stream density Ex: 1","title":"Environment Variables (EVs)"},{"location":"dev-tools/environment_variables.html#applying-ev-to-run-pipeline","text":"EV can be applied in two ways: 1. as parameter input to run . sh script 2. in the env files The input parameter will override the one in the env files if both are used.","title":"Applying EV to Run Pipeline"},{"location":"dev-tools/environment_variables.html#ev-as-input-parameter","text":"Example bash PIPELINE_PROFILE=\"object_detection\" CPU_ONLY=1 RENDER_MODE=0 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 Note Those EVs in front of run.sh like CPU_ONLY , RENDER_MODE are applied to this run only and they are also known as command line environment overrides, or environment overrides. They will override the default values in env files if any.","title":"EV as input parameter"},{"location":"dev-tools/environment_variables.html#editing-the-env-files","text":"EV can be configured for advanced user in configs/opencv-ovms/envs/ . As an example for gst pipeline profile, there are two Env files can be configured: - ` yolov5 - cpu . env ` file for running pipeline in core system - ` yolov5 - gpu . env ` file for running pipeline in gpu or multi These two files currently hold the default values.","title":"Editing the Env Files"},{"location":"dev-tools/overview.html","text":"Developer Overview","title":"Developer Overview"},{"location":"dev-tools/overview.html#developer-overview","text":"","title":"Developer Overview"},{"location":"dev-tools/references.html","text":"Libraries Library Link Intel\u00ae Deep Learning Streamer (Intel\u00ae DL Streamer) https://github.com/dlstreamer/dlstreamer gstreamer plugin for Intel\u00ae RealSense\u2122 Cameras https://github.com/G2020sudo/realsense-gstreamer Libraries for Intel\u00ae RealSense\u2122 Cameras https://github.com/gwen2018/librealsense Components Component Link gstreamer https://gstreamer.freedesktop.org/ Intel\u00ae RealSense\u2122 Technology https://www.intel.com/content/www/us/en/architecture-and-technology/realsense-overview.html","title":"References"},{"location":"dev-tools/references.html#libraries","text":"Library Link Intel\u00ae Deep Learning Streamer (Intel\u00ae DL Streamer) https://github.com/dlstreamer/dlstreamer gstreamer plugin for Intel\u00ae RealSense\u2122 Cameras https://github.com/G2020sudo/realsense-gstreamer Libraries for Intel\u00ae RealSense\u2122 Cameras https://github.com/gwen2018/librealsense","title":"Libraries"},{"location":"dev-tools/references.html#components","text":"Component Link gstreamer https://gstreamer.freedesktop.org/ Intel\u00ae RealSense\u2122 Technology https://www.intel.com/content/www/us/en/architecture-and-technology/realsense-overview.html","title":"Components"},{"location":"dev-tools/run_camera_simulator.html","text":"Camera Simulator Overview If you do not have a camera device plugged into the system, run the camera simulator to view the pipeline analytic results based on a sample video file to mimic real time camera video. You can also use the camera simulator to infinitely loop through a video file for consistent benchmarking. For example, if you want to validate whether the performance is the same for 6 hours, 12 hours, and 24 hours, looping the same video should produce the same results regardless of the duration. Run the Camera Simulator Follow the steps below to run the camera simulator: Download the video files to the sample-media directory: bash cd benchmark-scripts; ./download_sample_videos.sh; cd ..; Example This example downloads a sample video for 1080p@15fps. bash cd benchmark-scripts; ./download_sample_videos.sh 1920 1080 15; cd ..; Note Only AVC encoded files are supported. After the video files are downloaded to the sample-media folder, start the camera simulator: bash make run-camera-simulator Wait for few seconds, and then check if the camera-simulator containers are running: bash docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}' Success Your output is as follows: IMAGE STATUS NAMES openvino/ubuntu20_data_runtime:2021.4.2 Up 11 seconds camera-simulator0 aler9/rtsp-simple-server Up 13 seconds camera-simulator Note There could be multiple containers with the image \"openvino/ubuntu20_data_runtime:2021.4.2\", depending on the number of sample-media video files provided. Failure If all the Docker* containers are not visible, then review the console output for errors. Sometimes dependencies fail to resolve. Address obvious issues and retry. Stopping the Camera Simulator The camera simulator may be stopped and cleaned up by running bash make clean-simulator","title":"Camera Simulator"},{"location":"dev-tools/run_camera_simulator.html#camera-simulator","text":"","title":"Camera Simulator"},{"location":"dev-tools/run_camera_simulator.html#overview","text":"If you do not have a camera device plugged into the system, run the camera simulator to view the pipeline analytic results based on a sample video file to mimic real time camera video. You can also use the camera simulator to infinitely loop through a video file for consistent benchmarking. For example, if you want to validate whether the performance is the same for 6 hours, 12 hours, and 24 hours, looping the same video should produce the same results regardless of the duration.","title":"Overview"},{"location":"dev-tools/run_camera_simulator.html#run-the-camera-simulator","text":"Follow the steps below to run the camera simulator: Download the video files to the sample-media directory: bash cd benchmark-scripts; ./download_sample_videos.sh; cd ..; Example This example downloads a sample video for 1080p@15fps. bash cd benchmark-scripts; ./download_sample_videos.sh 1920 1080 15; cd ..; Note Only AVC encoded files are supported. After the video files are downloaded to the sample-media folder, start the camera simulator: bash make run-camera-simulator Wait for few seconds, and then check if the camera-simulator containers are running: bash docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}' Success Your output is as follows: IMAGE STATUS NAMES openvino/ubuntu20_data_runtime:2021.4.2 Up 11 seconds camera-simulator0 aler9/rtsp-simple-server Up 13 seconds camera-simulator Note There could be multiple containers with the image \"openvino/ubuntu20_data_runtime:2021.4.2\", depending on the number of sample-media video files provided. Failure If all the Docker* containers are not visible, then review the console output for errors. Sometimes dependencies fail to resolve. Address obvious issues and retry.","title":"Run the Camera Simulator"},{"location":"dev-tools/run_camera_simulator.html#stopping-the-camera-simulator","text":"The camera simulator may be stopped and cleaned up by running bash make clean-simulator","title":"Stopping the Camera Simulator"},{"location":"dev-tools/telemetry/setup.html","text":"Setup Telemetry Build Telegraf docker image bash make build-telegraf Run InfluxDB and Telegraf. Set password for InfluxDB as env variable in command line: Note Password must be at least 8 characters in length. bash make INFLUXPASS=yourpass run-telegraf Start the dashboard by opening a browser to http://127.0.0.1:8086 and login using username: telegraf and the password you set previously. Click on \"Build a Dashboard\", then \"Import dashboard\" and select the file intel_core_and_igpu_telemetry.json under telegraf folder. Run the dashboard","title":"Telemetry"},{"location":"dev-tools/telemetry/setup.html#setup-telemetry","text":"Build Telegraf docker image bash make build-telegraf Run InfluxDB and Telegraf. Set password for InfluxDB as env variable in command line: Note Password must be at least 8 characters in length. bash make INFLUXPASS=yourpass run-telegraf Start the dashboard by opening a browser to http://127.0.0.1:8086 and login using username: telegraf and the password you set previously. Click on \"Build a Dashboard\", then \"Import dashboard\" and select the file intel_core_and_igpu_telemetry.json under telegraf folder. Run the dashboard","title":"Setup Telemetry"},{"location":"release-notes/v1-0-1.html","text":"1.0.1 Intel\u00ae Automated Self-Checkout Reference Package 1.0.0 is the first major release. This release includes all items required to run the vision checkout pipeline and benchmarking. For details on running the solution, refer to the Overview . New Features Title Description Pipeline scripts Scripts that run the GStreamer-based vision checkout pipeline Benchmark scripts Scripts that start pipelines and system metrics based on parameters Docker* images Dockerized images for the pipeline and benchmark tools for code portability Set up Documentation Markdown files that include setup steps for initial use Unit tests Basic unit tests scripts for smoke testing Camera simulator Camera simulator script to simulate an RTSP stream using a media file Media downloader script Script to assist with downloading sample media for the camera simulator Model downloader script Script to assist with downloading the model files used for the pipelines Issues Fixed Issue Number Description Link None Initial Release Known Issues Issue Number Description Link 15 Pipeline core run on some HW is not producing inference results https://github.com/intel-retail/vision-self-checkout/issues/15 29 Unable to modify batch size from run script https://github.com/intel-retail/vision-self-checkout/issues/29","title":"1.0.1"},{"location":"release-notes/v1-0-1.html#101","text":"Intel\u00ae Automated Self-Checkout Reference Package 1.0.0 is the first major release. This release includes all items required to run the vision checkout pipeline and benchmarking. For details on running the solution, refer to the Overview .","title":"1.0.1"},{"location":"release-notes/v1-0-1.html#new-features","text":"Title Description Pipeline scripts Scripts that run the GStreamer-based vision checkout pipeline Benchmark scripts Scripts that start pipelines and system metrics based on parameters Docker* images Dockerized images for the pipeline and benchmark tools for code portability Set up Documentation Markdown files that include setup steps for initial use Unit tests Basic unit tests scripts for smoke testing Camera simulator Camera simulator script to simulate an RTSP stream using a media file Media downloader script Script to assist with downloading sample media for the camera simulator Model downloader script Script to assist with downloading the model files used for the pipelines","title":"New Features"},{"location":"release-notes/v1-0-1.html#issues-fixed","text":"Issue Number Description Link None Initial Release","title":"Issues Fixed"},{"location":"release-notes/v1-0-1.html#known-issues","text":"Issue Number Description Link 15 Pipeline core run on some HW is not producing inference results https://github.com/intel-retail/vision-self-checkout/issues/15 29 Unable to modify batch size from run script https://github.com/intel-retail/vision-self-checkout/issues/29","title":"Known Issues"},{"location":"release-notes/v1-5-0.html","text":"1.5.0 Intel\u00ae Automated Self-Checkout Reference Package 1.5.0 is the second main release. This release includes bug fixes, feature enhancements, dockerization of the benchmarking tools, and OpenVINO Model Server support. For details on running the solution, refer to the Overview . New Features Title Description OpenVINO Model Server OpenVINO Model Server support OpenVINO Model Server Pipelines Object detection pipelines using OpenVINO Model Server Benchmark scripts Dockerization Benchmark tools have been moved to Docker containers for more flexible deployment Github Build actions Code linting and security scans for pull requests Issues Fixed Issue Number Description 41 Pipeline failure log 42 Create makefile docker commands 51 Optimized density script to reduce run time on high powered systems 55 Make performance / powersave mode configurable 57 Add debug option for docker-run.sh 58 Doc update with makefile 61 rename vision self checkout to automated self checkout 65 Update documentation to include OVMS pipelines 66 Add model download top level script 67 [Tech Debt] Make --workload work in any option/argument position when run benchmark.sh 75 docker-run.sh with wrong message when no --workload option is provided 77 XPU Manager not running on multiple GPUs 85 Fix ShellCheck issues in scripts 88 Incorrect instructions for building IGT in pipelinebenchmarking.md 91 format avc mp4 tag logic is inverted 96 For ovms workload getModels.sh not working when it is called by docker-run.sh from project base directory 99 Clean up some checked in dlstreamer models 100 Add cleaning ovms containers to makefile 105 benchmark pcm directory incorrect 109 igt path pointing to the incorrect directory causing the igt log to not be written 112 make CPU as default device for ovms pipeline 115 add dockerfile.bitModel to download bit models 119 pipelinesetup doc has incorrect link to models.list.yml 124 add ovms sample image download into run script 129 Update License to Apache 2.0 131 update mkdoc to navigate to OVMS doc 142 make build-ovms-server failed for 2nd time or later after removed the Docker image for rebuild Known Issues Issue Number Description Link None","title":"1.5.0"},{"location":"release-notes/v1-5-0.html#150","text":"Intel\u00ae Automated Self-Checkout Reference Package 1.5.0 is the second main release. This release includes bug fixes, feature enhancements, dockerization of the benchmarking tools, and OpenVINO Model Server support. For details on running the solution, refer to the Overview .","title":"1.5.0"},{"location":"release-notes/v1-5-0.html#new-features","text":"Title Description OpenVINO Model Server OpenVINO Model Server support OpenVINO Model Server Pipelines Object detection pipelines using OpenVINO Model Server Benchmark scripts Dockerization Benchmark tools have been moved to Docker containers for more flexible deployment Github Build actions Code linting and security scans for pull requests","title":"New Features"},{"location":"release-notes/v1-5-0.html#issues-fixed","text":"Issue Number Description 41 Pipeline failure log 42 Create makefile docker commands 51 Optimized density script to reduce run time on high powered systems 55 Make performance / powersave mode configurable 57 Add debug option for docker-run.sh 58 Doc update with makefile 61 rename vision self checkout to automated self checkout 65 Update documentation to include OVMS pipelines 66 Add model download top level script 67 [Tech Debt] Make --workload work in any option/argument position when run benchmark.sh 75 docker-run.sh with wrong message when no --workload option is provided 77 XPU Manager not running on multiple GPUs 85 Fix ShellCheck issues in scripts 88 Incorrect instructions for building IGT in pipelinebenchmarking.md 91 format avc mp4 tag logic is inverted 96 For ovms workload getModels.sh not working when it is called by docker-run.sh from project base directory 99 Clean up some checked in dlstreamer models 100 Add cleaning ovms containers to makefile 105 benchmark pcm directory incorrect 109 igt path pointing to the incorrect directory causing the igt log to not be written 112 make CPU as default device for ovms pipeline 115 add dockerfile.bitModel to download bit models 119 pipelinesetup doc has incorrect link to models.list.yml 124 add ovms sample image download into run script 129 Update License to Apache 2.0 131 update mkdoc to navigate to OVMS doc 142 make build-ovms-server failed for 2nd time or later after removed the Docker image for rebuild","title":"Issues Fixed"},{"location":"release-notes/v1-5-0.html#known-issues","text":"Issue Number Description Link None","title":"Known Issues"}]}